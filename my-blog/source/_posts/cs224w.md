---
title: CS224w笔记
date: 2023-04-28 14:00:00
tags:
  - graph
  - AI生成 
categories:
  - 笔记
---
### 引言 (Introduction)

#### 0.1 为什么需要图 (Why Graphs?)

*   **定义:** 图 (Graph) 是一种用于描述和分析包含实体及其之间关系/交互的通用语言。节点 (Node/Vertex) 代表实体，边 (Edge/Link) 代表它们之间的关系或交互。
*   **解释:** 许多复杂领域的数据具有丰富的关系结构，如社交网络、分子结构、知识图谱、计算机网络、生物通路等。传统机器学习工具箱（处理向量、序列、网格如图像/文本）难以直接捕捉这种关系结构。图提供了一种强大的方式来显式地建模这些关系，从而可能以较低的模型复杂度获得更好的性能。图连接万物 (Graphs connect things)。
*   **重要例子:**
    *   **分子 (Molecules):** 原子是节点，化学键是边。
    *   **社交网络:** 人是节点，朋友/关注关系是边。
    *   **计算机网络:** 设备是节点，连接是边。
    *   **知识图谱:** 实体（人、地点、概念）是节点，它们之间的关系（如出生在、位于）是边。
*   **关联:** 图学习的核心在于如何利用图的结构信息进行预测。图神经网络 (GNN) 是近年来处理图数据的热门技术。

#### 图学习任务 (Tasks on Graph Data)

*   **解释:** 可以在图数据上执行多种类型的机器学习任务，根据预测目标的不同层级划分。
*   **类型:**
    *   **节点级别预测 (Node-level prediction):** 预测图中单个节点的属性或类别。关注节点的结构和在网络中的位置。
        *   **例子:** 预测蛋白质结构中原子的坐标；对社交网络用户进行分类（如机器人检测）。
    *   **边/链接级别预测 (Edge/Link-level prediction):** 预测一对节点之间是否存在边，或预测边的属性。可以是发现缺失的链接或预测未来可能形成的链接。
        *   **例子:** 推荐系统（预测用户可能喜欢的物品）；药物副作用预测（预测药物之间的相互作用）。
    *   **图级别预测 (Graph-level prediction):** 预测整个图（或子图）的属性或标签。
        *   **例子:** 分子属性预测（如毒性、溶解度）；交通流量预测；物理模拟；天气预报。
*   **关联:** 不同的任务需要不同的模型设计和输出层。[[图嵌入 (Graph Embeddings)]] 和 [[图神经网络 (Graph Neural Networks)]] 可以应用于所有这些级别的任务。

#### 0.2 图嵌入的选择 (Choice of Graph Embeddings)

*   **定义:** 图嵌入 (Graph Embedding) 是将图的组件（节点、边、整个图）映射到低维向量空间（通常是欧氏空间）的过程，同时保留图的结构信息和属性。
*   **解释:** 选择如何表示图（即如何定义节点、边和它们的属性）至关重要，这决定了能从图中挖掘出哪些信息。图的表示并非总是唯一的。
*   **图的基本组件:**
    *   **对象 N (Objects):** 节点 (Nodes) / 顶点 (Vertices)。
    *   **交互 E (Interactions):** 边 (Edges) / 链接 (Links)。
    *   **系统 G(N, E) (Systems):** 网络 (Networks) / 图 (Graphs)。
*   **图的属性/类型:**
    *   **有向/无向 (Directed/Undirected):** 边是否有方向。
    *   **自环 (Self-loop):** 节点到自身的边是否允许。
    *   **多重图 (Multi-graph):** 节点之间是否允许多条边。
    *   **异构图 (Heterogeneous Graphs):** 节点或边具有不同类型。G = (V, E, R, T)，边有关系类型 `r ∈ R`，节点有类型 `T(vi)`。
        *   **例子:** 药物-蛋白质相互作用图；文献引用网络（作者、论文、会议）。
    *   **二分图 (Bipartite Graphs):** 节点可以分为两个集合，所有边只连接两个集合之间的节点。
        *   **例子:** 作者-论文图；演员-电影图；用户-物品交互图。
*   **图的稀疏性:** 真实世界的网络通常是稀疏的，即边的数量远小于节点数量的平方 (|E| ≪ |V|²)。邻接矩阵是稀疏矩阵。
*   **关联:** 图的类型（同构/异构，有向/无向等）会影响特征工程、嵌入方法和GNN架构的选择。例如，[[关系型GCN (Relational GCN)]] 用于处理异构图。




### 1 传统的图机器学习 (Traditional Machine Learning on Graphs)

*   **解释:** 在深度学习（特别是GNN）普及之前，对图进行机器学习通常依赖于手动设计的特征（特征工程）。这些特征捕捉图的结构信息，然后输入到传统的机器学习模型（如逻辑回归、随机森林）中。关键在于设计有效的图特征。本节主要关注无向图。

#### 1.1 节点级别特征 (Node-Level Features)

*   **目标:** 用于节点级别的任务，如节点分类。特征需要能表征节点在图中的结构和位置。
*   **特征类型:**
    *   **节点度 (Node Degree):** 节点的邻居数量 `kv`。最简单的结构特征。
    *   **节点中心性 (Node Centrality):** 衡量节点在图中的“重要性”。
        *   **特征向量中心性 (Eigenvector Centrality):** `cv := (1/λ) * Σ(u∈N(v)) cu` 或 `λc = Ac`。一个节点的重要性取决于其邻居的重要性。由邻接矩阵 `A` 的最大特征值 `λmax` 对应的特征向量 `c` 给出。
        *   **介数中心性 (Betweenness Centrality):** `cv := Σ(s≠v≠t) |{经过v的最短路径数}| / |{s,t间最短路径总数}|`。衡量节点作为“守门员”或桥梁的重要性，即它在多少节点对的最短路径上。对社交网络很重要。
        *   **接近度中心性 (Closeness Centrality):** `cv := 1 / Σ(u≠v) |{u,v间最短路径长度}|`。衡量节点到图中所有其他节点的平均距离的倒数，表示节点到达其他节点的难易程度。
    *   **聚类系数 (Clustering Coefficient):** `ev := (1 / (kv choose 2)) * |{N(v)中的边数}| ∈ [0, 1]`。衡量节点的邻居之间相互连接的紧密程度（即邻居构成团的程度）。计算节点自我中心网络 (ego-network) 中三角形的数量。社交网络通常有很高的聚类系数。
    *   **图元 (Graphlets):** 小的、连通的、诱导的、非同构的子图。用于描述节点邻域的更复杂的拓扑结构。
        *   **图元度向量 (Graphlet Degree Vector - GDV):** 一个向量，其元素表示包含该节点的特定图元（ rooted at the node）的数量。例如，使用2到5个节点的图元，可以得到一个73维的GDV向量。
*   **局限性:** 这些传统特征主要捕捉局部拓扑属性，可能无法区分全局尺度上不同的节点。需要手动设计，缺乏通用性。
*   **诱导子图 (Induced Graph):** 由原图中节点子集以及所有连接该子集中节点的边构成的子图。
*   **同构 (Isomorphic):** 两个图如果拓扑结构完全相同（可以通过节点重新标记相互转换），则它们是同构的。
*   **关联:** 这些手动特征是[[节点嵌入 (Node Embeddings)]] 和 [[图神经网络 (GNNs)]] 试图自动学习和超越的基线。GNN的聚合过程可以看作是在学习类似这些特征的表示。

#### 1.2 链接级别特征 (Link-Level Features)

*   **目标:** 用于链接预测任务，预测一对节点之间是否存在链接。
*   **两种设定:**
    *   **随机缺失链接:** 随机移除一些边，然后预测它们。
    *   **随时间演化的链接:** 基于过去 (`G[t0, t'0]`) 的图结构，预测未来 (`G[t1, t'1]`) 会出现的链接。评估时，计算预测得分 `c(x, y)`，排序后取 top n 作为预测链接，与测试期间实际出现的新链接 `Enew` 比较。
*   **特征类型 (基于局部邻域重叠):**
    *   **共同邻居 (Common Neighbors):** `c(u, v) := |N(u) ∩ N(v)|`。
    *   **Jaccard 系数 (Jaccard's Coefficient):** `c(u, v) := |N(u) ∩ N(v)| / |N(u) ∪ N(v)|`。共同邻居数除以总邻居数。
    *   **Adamic-Adar 指数 (Adamic-Adar Index):** `c(u, v) := Σ(w∈N(u)∩N(v)) 1 / log(kw)`。共同邻居的重要性加权求和，度数低的共同邻居贡献更大。
*   **局限性 (局部特征):** 上述三个指标仅在 `u, v` 有共同邻居时才非零。
*   **特征类型 (基于全局路径):**
    *   **Katz 指数 (Katz Index):** `c(u, v) := Σ(l=1 to ∞) β^l * |{u,v间长度为l的路径数}|`。计算所有长度路径的数量，并用衰减因子 `β < 1` 加权。可以通过邻接矩阵的幂 `A^n` 计算长度为 `n` 的路径数，Katz 指数矩阵 `C = Σ(i=1 to ∞) β^i * A^i = (I - βA)^(-1) - I`。考虑了全局路径信息。
*   **关联:** 这些特征也是 [[节点嵌入]] 方法（如 node2vec）试图隐式捕捉的相似性度量基础。链接预测是GNN的一个重要应用。

#### 1.3 图核 (Graph Kernels)

*   **目标:** 用于图级别的任务，如图分类。旨在为整个图创建一个特征向量或定义一个核函数。
*   **核方法 (Kernel Methods):** 无需显式设计特征向量 `ϕ(G)`，而是设计一个核函数 `k(G, G') ≈ ϕ(G)·ϕ(G')`，该函数衡量两个图 `G` 和 `G'` 之间的相似性。核矩阵 `K = [k(G, G')]` 必须是半正定的。
*   **方法示例:**
    *   **词袋模型 (Bag-of-Words - BoW):** 将图中的节点（或子结构）视为“单词”，计算其出现次数作为特征。忽略了结构信息。
    *   **图级图元特征 (Graph-Level Graphlet Features):** 计算图中不同（非根植、不必连通）图元的数量。
        *   **局限性:** 计算成本高。枚举大小为 `k` 的图元需要 `O(n^k)` 时间，因为涉及子图同构测试（NP难问题）。对于度有界的图，可以优化到 `O(n * d^(k-1))`。
*   **局限性:** 图核方法通常计算成本高，且难以结合节点/边的属性信息。
*   **关联:** [[图神经网络]] 通过端到端学习可以直接处理图级别任务，通常比图核方法更有效、更灵活。




### 2 节点嵌入 (Node Embeddings)

*   **目标:** 自动学习节点的低维向量表示（嵌入），使得向量空间中的相似性（如点积、距离）能够反映原始图中节点的相似性（如网络结构上的接近度、邻域相似性等）。避免了手动特征工程。
*   **核心思想:** 定义一个编码器 (Encoder) 将节点映射到嵌入向量，定义一个解码器 (Decoder) 从嵌入向量计算节点间的相似度，并定义一个目标相似度函数，使得 `similarity(u, v) ≈ Decoder(Encoder(u), Encoder(v))`。
*   **期望属性:**
    *   嵌入向量相似性反映网络相似性。
    *   编码网络信息。
    *   对下游任务有用（如节点分类、链接预测）。

#### 框架 (Encoder-Decoder Framework)

*   **编码器 (Encoder):** `Enc: V -> R^d`。将节点 `u` 映射到 `d` 维嵌入向量 `zu`。
    *   **浅层编码器 (Shallow Encoder):** 最简单的形式是一个查找表（嵌入矩阵 `Z`，大小为 `d x |V|`），每个节点对应一个独立的向量。`Enc(u) = zu` 就是直接查找 `Z` 中的对应列。这是唯一需要学习的部分。
    *   **深度编码器 (Deep Encoder):** 使用 [[图神经网络 (GNNs)]] 进行编码（见后续章节）。
*   **解码器 (Decoder):** `Dec: R^d x R^d -> R+`。根据两个节点的嵌入计算它们的相似度。
    *   **常用选择:** 点积 `Dec(z, w) = z · w`。
*   **相似度函数 (Similarity Function):** 定义图中节点 `u, v` 之间的“真实”相似度 `similarity(u, v)`。
    *   **例子:** 基于图的距离、邻域重叠、随机游走概率等。
*   **目标:** 学习编码器参数（对于浅层编码器就是嵌入矩阵 `Z`），使得 `similarity(u, v) ≈ Dec(Enc(u), Enc(v))`。
*   **代表方法:** DeepWalk, node2vec。

#### 2.1 随机游走嵌入 (Random Walk Embedding)

*   **核心思想:** 使用随机游走来定义节点间的相似度。如果节点 `u` 的随机游走路径经常访问节点 `v`，则认为 `u` 和 `v` 相似。这是一种无监督/自监督的方法，不依赖节点标签或特征。
*   **优势:**
    *   **表达能力 (Expressivity):** 灵活捕捉局部和高阶邻域信息。
    *   **效率 (Efficiency):** 无需遍历整个图即可训练。
*   **步骤:**
    1.  **随机游走:** 从每个节点开始，执行短的、固定长度的随机游走。
    2.  **收集邻域:** 对每个节点 `u`，收集其随机游走经过的节点集合 `NR(u)`（这是一个多重集，可以包含重复节点）。`NR(u)` 代表了 `u` 的（随机游走意义下的）邻域。
    3.  **优化目标 (损失函数):** 最大化给定节点 `u` 的嵌入 `zu` 后，其邻域节点 `v ∈ NR(u)` 出现的条件概率 `P(v|zu)`。通常使用负对数似然损失：
        `L(Enc) = - Σ(u∈V) Σ(v∈NR(u)) log P(v|zu)`
    4.  **参数化概率:** 使用 Softmax 函数将嵌入的点积转换为概率：
        `P(v|zu) = exp(zu · zv) / Σ(n∈V) exp(zu · zn)`
    5.  **优化挑战:** Softmax 的分母计算成本高 (`O(|V|^2)`)。
    6.  **负采样 (Negative Sampling):** 解决 Softmax 计算瓶颈。近似 `log P(v|zu)` 为：
        `log σ(zu · zv) - Σ(k i=1) log σ(zu · z_ni)`，其中 `ni` 是从某个噪声分布 `Pv` (如按节点度的 3/4 次方采样) 中采样的 `k` 个负样本节点。`k` 通常取 5 到 20。
    7.  **优化:** 使用随机梯度下降 (SGD) 优化编码器参数 `Enc` (即嵌入向量 `zu`)。
*   **随机游走策略 R:**
    *   **DeepWalk:** 使用无偏（均匀）随机游走。
    *   **node2vec:** 引入带偏置的二阶随机游走，以平衡 BFS 和 DFS 探索。
        *   **参数:**
            *   返回参数 `p`: 控制回到前一个节点的概率 (1/p)。低 `p` 使游走倾向于停留在局部 (BFS-like)。
            *   进出参数 `q`: 控制探索远处节点 (DFS-like) 与访问近处节点 (BFS-like) 的概率。低 `q` 鼓励向外探索 (DFS-like)。
        *   **效果:** node2vec 在节点分类上表现较好，而其他方法可能在链接预测上更优。随机游走方法通常效率较高。

#### Excursion: 随机梯度下降 (Stochastic Gradient Descent - SGD)

*   **解释:** 一种优化算法，用于最小化损失函数。在每次迭代中，不是计算整个训练集的梯度，而是基于一小批 (mini-batch) 样本计算梯度并更新参数。
*   **在随机游走嵌入中的应用:**
    1.  随机初始化所有 `zu`。
    2.  循环直到收敛：
        *   对于每个节点 `u`（或一批节点）：
            *   执行随机游走生成 `NR(u)`。
            *   计算损失 `L` 对 `zu` 的梯度 `∂L/∂zu`（使用负采样）。
            *   更新 `zu ← zu - η * ∂L/∂zu` (η 是学习率)。

#### 2.2 嵌入整个图 (Embedding Entire Graphs)

*   **目标:** 为整个图（或子图）生成一个嵌入向量 `zG`。
*   **方法:**
    *   **简单求和/平均:** 对图中所有节点的嵌入向量 `zv` 进行求和或平均：`zG := Σ(v∈G) zv`。
        *   **局限性:** 对于大图可能会丢失信息。
    *   **引入虚拟节点:** 添加一个代表整个图的虚拟节点，然后运行节点嵌入算法，使用该虚拟节点的嵌入作为图嵌入。
    *   **分层嵌入 (Hierarchical Embeddings):** 如 DiffPool。通过 GNN 不断将节点聚类，生成越来越粗化的图，最终得到整个图的嵌入。在每一层：
        *   GNN-A 计算节点嵌入。
        *   GNN-B 计算节点到聚类的分配。
        *   基于聚类分配聚合 GNN-A 的节点嵌入，形成下一层的粗化图节点。
*   **关联:** 图嵌入是图级别预测任务的基础。

#### 2.3 与矩阵分解的关系 (Relations to Matrix Factorization)

*   **解释:** 浅层节点嵌入可以看作是一种矩阵分解。
*   **编码器:** 嵌入查找表可以视为一个矩阵 `Z` (大小 `d x |V|`)。
*   **解码器:** 点积解码器 `zu · zv` 对应于 `Z^T Z` 矩阵的元素。
*   **目标相似度:**
    *   如果目标是邻接矩阵 `A`，则优化 `min_Z ||A - Z^T Z||_F` 就是一种矩阵分解。
    *   DeepWalk 的嵌入实际上是在分解一个与随机游走相关的矩阵（包含了节点度和高阶邻近信息）：`log(vol(G) * (1/T * Σ(r=1 to T) (D^-1 A)^r) * D^-1) - log b`。其中 `D^-1 A` 是随机游走的转移矩阵。
*   **关联:** 提供了理解浅层嵌入方法的另一种视角，将其与经典的降维技术联系起来。

#### 2.4 应用和局限性 (Applications and Limitations)

*   **应用:**
    *   聚类/社区发现
    *   节点分类
    *   链接预测: 基于 `(zi, zj)` 预测边 `(i, j)`。
    *   图分类: 基于 `zG` 进行分类。
*   **局限性:**
    *   **参数量大 (O(|V|d)):** 每个节点都需要独立的嵌入向量，无法在节点间共享参数。
    *   **直推式 (Transductive):** 只能为训练时见过的节点生成嵌入，无法处理新节点（图结构变化时需要重新训练）。
    *   **无法捕捉结构相似性:** 不能保证结构相似（如同构邻域）但身份不同的节点具有相似的嵌入。
    *   **无法利用节点/边/图特征:** 浅层嵌入通常不直接使用节点的属性特征。
*   **关联:** [[图神经网络 (GNNs)]] 旨在克服这些局限性，实现参数共享、归纳式学习并融合节点/边特征。




### 3 图神经网络 (Graph Neural Networks - GNNs)

*   **动机:** 克服传统图特征工程和浅层嵌入的局限性，开发能够端到端学习图表示的深度学习模型。GNN 使用深度图编码器 (Deep Graph Encoder)。
*   **挑战:** 图结构复杂，不像图像（规则网格）或文本（序列）那样规整。节点没有固定顺序，邻居数量可变，图可能是动态的。

#### 3.1 深度学习基础 (Basics of Deep Learning)

*   **Excursion: 监督学习 (Supervised Learning)**
    *   **目标:** 给定输入 `x` 预测输出 `y`。
    *   **优化问题:** `min_θ L(y, f(x; θ))`，其中 `f` 是模型（参数为 `θ`），`L` 是损失函数。
    *   **损失函数 (Loss Function):**
        *   **L2 损失:** `||y - ŷ||` (回归)。
        *   **交叉熵 (Cross Entropy):** `- Σ yi log(ŷi)` (分类)。
    *   **优化:** 使用梯度下降法 (`θ ← θ - η ∇θ L`)。`∇θ L` 是梯度，`η` 是学习率。
    *   **随机梯度下降 (SGD):** 使用小批量 (mini-batch) 样本 `I` 近似梯度 `∇θ L ≈ Σ(i∈I) ∇θ L(yi, f(xi; θ))`。
        *   **Batch Size:** `|I|`。
        *   **Epoch:** 对整个数据集的一次完整遍历。
        *   **优化器 (Optimizers):** SGD, Adam, RMSprop 等。
    *   **多层感知机 (Multi-Layer Perceptron - MLP):** 由多个线性层 `σ(Wx + b)` 堆叠而成。`W, b` 是可学习参数，`σ` 是非线性激活函数 (Activation Function)。

#### 3.2 图的深度学习 (Deep Learning for Graphs)

*   **朴素方法的局限:** 直接将邻接矩阵 `A` 和特征矩阵 `X` 输入 MLP 会遇到问题：参数量 `O(|V|)`，对节点顺序敏感，无法处理不同大小的图。
*   **卷积网络 (Convolutional Networks) 的启发:** CNN 使用在空间上共享的、滑动的卷积核来处理图像网格。但在图上没有固定的邻域结构和顺序。
*   **置换不变性/等变性 (Permutation Invariance/Equivariance):**
    *   **置换矩阵 (Permutation Matrix) P:** 用于重新排列节点顺序。`A' = PAP^T`, `X' = PX`。
    *   **置换不变 (Permutation Invariant):** 函数 `f` 满足 `f(A, X) = f(PAP^T, PX)`。例如，图级别预测函数应该是置换不变的。
    *   **置换等变 (Permutation Equivariant):** 函数 `f` 满足 `Pf(A, X) = f(PAP^T, PX)`。例如，节点级别预测或 GNN 的中间层应该是置换等变的。
*   **GNN 的核心:** GNN 由多个置换等变/不变的层组成，能够处理任意图结构和节点顺序。

#### 3.3 图卷积网络 (Graph Convolutional Networks - GCN)

*   **核心思想:** 每个节点的表示（嵌入）是其邻域信息的聚合和转换。信息在计算图上传播。
*   **计算图 (Computation Graph):** 每个节点 `v` 都定义了一个基于其 `k`-hop 邻域的计算图。不同节点的计算图结构可能不同。
*   **邻域聚合 (Neighborhood Aggregation):** GNN 层通过聚合邻居节点上一层的表示来计算当前节点的表示。
*   **基本 GCN 层 (Deep Encoder):**
    *   **第 0 层:** 节点 `v` 的初始嵌入是其输入特征 `h_v^(0) := x_v`。
    *   **第 k 层:**
        `h_v^(k+1) := σ( W_k * AGG({h_u^(k) | u ∈ N(v)}) + B_k * h_v^(k) )` (公式简化形式)
        更常见的形式是：
        `h_v^(k+1) := σ( Σ(u∈N(v)∪{v}) (1/c_{vu}) * W_k * h_u^(k) )` (其中 `c_{vu}` 是归一化常数)
        或者写成矩阵形式：
        `H^(k+1) := σ( D̃^(-1/2) Ã D̃^(-1/2) H^(k) W_k^T )` (原始 GCN 公式，其中 `Ã = A+I`, `D̃` 是 `Ã` 的度矩阵)
        简化形式（如课程中推导）：
        `H^(k+1) := σ( D^(-1) A H^(k) W_k^T + H^(k) B_k^T )` (如果分别处理邻居和自身)
    *   **组件:**
        *   `h_v^(k)`: 节点 `v` 在第 `k` 层的嵌入。
        *   `AGG`: 聚合函数 (如求和、均值、最大值)。
        *   `W_k`, `B_k`: 第 `k` 层的可学习权重矩阵（参数共享，与节点无关）。
        *   `σ`: 非线性激活函数 (如 ReLU)。
    *   **最终嵌入:** 节点 `v` 的最终嵌入是第 `K` 层的输出 `z_v := h_v^(K)`。
*   **置换等变性:** GCN 的计算过程是置换等变的。
*   **训练 GNN:**
    *   **监督学习:** 定义损失函数 `L(y_v, f(z_v))` (其中 `f` 是最终预测层)，使用 SGD 优化 GNN 参数 `W_k, B_k` 和预测层参数。
        *   **例子 (节点二分类):** `L = - Σ(v∈V_labeled) [ yv log(σ(z_v^T θ)) + (1 - yv) log(1 - σ(z_v^T θ)) ]`
    *   **无监督/自监督学习:** 利用图结构作为监督信号。例如，要求图中相近的节点 `u, v` 具有相似的嵌入 `z_u, z_v`。可以使用类似 [[随机游走嵌入]] 的损失函数，或基于边的重构损失。
        *   **例子:** `L = Σ(u,v) CE(y_u,v, Dec(z_u, z_v))`，其中 `y_u,v` 是某种节点相似度得分。

#### 3.4 GNNs 包含 CNNs (GNNs subsume CNNs)

*   **解释:** CNN 可以看作是一种特殊的 GNN。
*   **类比:** 图像可以看作是一个规则的网格图，每个像素是一个节点，连接到其 8 个邻居像素。
*   **CNN 作为 GNN:** CNN 的卷积操作可以写成 GNN 的邻域聚合形式：
    `h_v^(l+1) := σ( Σ(u∈N(v)∪{v}) w_{l,u} * h_u^(l) )`
    其中 `N(v)` 是像素 `v` 的 8 个邻居，`w_{l,u}` 是卷积核对应位置的权重。
*   **关键区别:**
    *   **GNN:** 邻居数量可变，没有固定顺序，权重通常是共享的（如 `W_k`）。
    *   **CNN:** 邻居数量固定 (如 3x3)，邻居有固定相对位置，可以为每个相对位置学习不同的权重 `w_{l,u}`。CNN 不是置换不变/等变的。
*   **结论:** GNN 是比 CNN 更通用的架构。

#### GNN 总体流程

1.  定义邻域聚合函数（选择 GNN 模型）。
2.  定义嵌入上的损失函数（根据任务）。
3.  在一系列节点（一个批次的计算图）上训练 GNN。
4.  生成节点嵌入（可应用于训练中未见过的节点）。

*   **归纳式学习 (Inductive):** GNN 模型学习到的聚合函数可以应用于新的、未见过的节点或图，因为参数 `W_k, B_k` 是共享的，与特定节点无关。这与浅层嵌入的直推式 (Transductive) 不同。




### 4 图神经网络通用视角 (A General Perspective on Graph Neural Networks)

*   **目标:** 将 GNN 的设计分解为模块化的组件，提供一个更通用的框架来理解和设计 GNN。
*   **GNN 框架的 5 个核心部分:**
    1.  **消息 (Message):** 定义节点如何产生传递给邻居的信息。
    2.  **聚合 (Aggregation):** 定义节点如何整合来自邻居的消息。 (消息+聚合构成一个 GNN 层)
    3.  **层连接 (Layer Connectivity):** 定义不同 GNN 层之间如何连接（如顺序堆叠、跳跃连接）。
    4.  **图增强 (Graph Augmentation):** 对原始输入图进行转换或增强（特征增强、结构增强）。
    5.  **学习目标 (Learning Objective):** 定义模型需要优化的任务（监督/无监督，节点/边/图级别）。

#### 4.1 单层 GNN (A Single Layer of GNN)

*   **输入:** 节点 `v` 上一层的隐藏状态 `h_v^(l-1)` 及其邻居的隐藏状态 `{h_u^(l-1) | u ∈ N(v)}`。
*   **输出:** 节点 `v` 当前层的嵌入 `h_v^(l)`。
*   **计算过程:**
    1.  **消息计算 (Message Computation):** 每个邻居 `u`（有时包括节点 `v` 自身）计算发送给 `v` 的消息 `m_u^(l)`。
        `m_u^(l) := Msg^(l)(h_u^(l-1))`
        *   **例子:** 线性消息 `m_u^(l) := W^(l) h_u^(l-1)`。
        *   **问题:** 节点是否能向不同邻居发送不同消息？可以，例如 [[图注意力网络 (GAT)]]。
    2.  **消息聚合 (Message Aggregation):** 节点 `v` 聚合所有收到的消息。
        `h_v^(l) := Agg({m_u^(l) | u ∈ N(v)})`
        *   **聚合函数 (Aggregation Function) Agg:** 必须是置换不变量函数，处理可变数量的输入。
        *   **例子:** 求和 (Sum), 均值 (Mean), 最大化 (Max)。
    3.  **更新 (Update):** （可选）结合聚合后的消息和节点自身上一层的表示来更新当前层的表示。为了避免丢失节点自身信息 (`h_v^(l-1)`), 通常会将聚合邻居信息与自身信息结合：
        `h_v^(l) := Update(h_v^(l-1), Agg({m_u^(l) | u ∈ N(v)}))`
        或者，将自身视为一个特殊的邻居进行消息计算和聚合：
        `h_v^(l) := Agg({m_u^(l) | u ∈ N(v) ∪ {v}})`
        或者，先聚合邻居消息，再与自身消息（或表示）合并：
        `h_v^(l) := concat(Agg({m_u^(l) | u ∈ N(v)}), m_v^(l))` (GraphSAGE 风格)
    4.  **激活函数 (Activation):** 应用非线性激活函数 `σ` 增加模型表达能力。可以在消息计算后或聚合/更新后应用。

#### GNN 层实例

*   **(1) 图卷积网络 (GCN):**
    *   **消息:** `m_u^(l) = (1 / deg(v)) * W^(l) h_u^(l-1)` (消息包含了归一化)
    *   **聚合:** 求和 `Σ`
    *   **更新:** `h_v^(l) = σ( Σ(u∈N(v)) m_u^(l) )` (假设包含自环)
    *   **特点:** 使用了基于度的归一化，假设有自环。
*   **(2) GraphSAGE (Graph SAmple and aggreGatE):**
    *   **消息:** `Msg(h_u^(l-1))` (消息在聚合函数内部计算)
    *   **聚合 (多种选择):**
        *   **均值 (Mean):** `Agg({h_u^(l-1)}) = mean({h_u^(l-1)})`
        *   **池化 (Pool):** 对邻居表示 `h_u^(l-1)` 先用 MLP 转换，然后应用对称函数 (mean/max)。`Agg = mean/max({MLP(h_u^(l-1))})`
        *   **LSTM:** 将邻居随机排序后输入 LSTM。`Agg = LSTM([h_u^(l-1)])`
    *   **更新:** `h_v^(l) = σ( W^(l) · concat(h_v^(l-1), Agg({h_u^(l-1) | u ∈ N(v)})) )`
    *   **特点:** 先聚合邻居，再与自身表示拼接，最后通过线性层和激活函数。聚合函数选择多样。可选 L2 归一化 `h_v^(l)`。
*   **(3) 图注意力网络 (GAT - Graph Attention Networks):**
    *   **核心思想:** 聚合邻居时，使用注意力机制 (Attention Mechanism) 为不同邻居分配不同的重要性权重 `α_{v,u}`。
    *   **注意力系数 (Attention Coefficients) `e_{v,u}`:** 衡量邻居 `u` 的消息对节点 `v` 的重要性。通常由 `u` 和 `v` 的表示计算得到。
        `e_{v,u} := a(W^(l) h_u^(l-1), W^(l) h_v^(l-1))` (a 是一个可学习的注意力函数，如单层 MLP)
    *   **注意力权重 (Attention Weights) `α_{v,u}`:** 对 `e_{v,u}` 在 `v` 的所有邻居上进行 Softmax 归一化。
        `α_{v,u} := softmax_u(e_{v,u}) = exp(e_{v,u}) / Σ(k∈N(v)) exp(e_{v,k})`
    *   **消息:** `m_u^(l) = W^(l) h_u^(l-1)` (这里是线性消息)
    *   **聚合/更新:** 对消息进行加权求和。
        `h_v^(l) = σ( Σ(u∈N(v)) α_{v,u} * m_u^(l) )`
    *   **多头注意力 (Multi-head Attention):** 并行计算多组 (`J` 组) 注意力权重 `α_{v,u}[j]` 和消息，然后将结果聚合（如拼接或平均）。
        `h_v^(l)[j] = σ( Σ(u∈N(v)) α_{v,u}[j] * W_j^(l) h_u^(l-1) )`
        `h_v^(l) = Agg_heads({h_v^(l)[j] | j=1..J})`
    *   **优点:** 隐式学习邻居重要性，计算高效（可并行），存储高效（稀疏操作），局部化，具有归纳能力。

#### 4.2 实践中的 GNN 层 (GNN Layers in Practice)

*   **超越经典层:** 实践中常通过添加现代深度学习模块来提升 GNN 层性能。
*   **常用模块:**
    *   **批量归一化 (Batch Normalization):** 稳定训练过程。
    *   **Dropout:** 防止过拟合（通常应用于消息函数中的线性层）。
    *   **注意力/门控 (Attention/Gating):** 控制信息流动。
    *   **激活函数:** ReLU 是常用选择，但 Parametric ReLU (PReLU) `PReLU(x) = max(x, 0) + a min(x, 0)` (a 可学习) 或 LeakyReLU, Swish 等有时效果更好。
*   **设计新 GNN 层:** 仍然是一个活跃的研究领域 (如 GraphGym 平台)。

#### 4.3 堆叠 GNN 层 (Stacking GNN Layers)

*   **标准方式:** 顺序堆叠 GNN 层，`H^(l+1) = GNN_Layer(H^(l))`。
*   **过平滑问题 (Over-smoothing Problem):** 随着 GNN 层数增加，节点的感受野 (receptive field) 不断扩大。最终，图中任意两个节点的感受野会大量重叠甚至相同，导致它们的嵌入趋于一致，失去区分性。
*   **应对策略:**
    1.  **谨慎堆叠:** GNN 层数不宜过多，通常略大于图半径即可。与传统 DNN 不同，更深的 GNN 不一定更好。
    2.  **增强单层表达能力:** 将 GNN 层内部的聚合/转换函数设计为更深的神经网络（如 MLP）。
    3.  **添加非消息传递层:** 在 GNN 层前后添加 MLP 层（逐节点应用），作为预处理 (pre-process) 和后处理 (post-process) 层。这些层不进行邻域聚合。实践中很有益。
    4.  **跳跃连接 (Skip Connections):** 允许信息从浅层直接传递到深层，创建浅层模型和深层模型的混合体。
        *   **残差连接 (Residual Connection) 形式:**
            `h_v^(l) := σ( Agg({m_u^(l)}) + h_v^(l-1) )`
        *   **直接连接到最后一层:** 将所有中间层的嵌入拼接或聚合后输入最终预测层。

#### 4.4 GNN 中的图操作 (Graph Manipulation in GNNs)

*   **动机:** 原始输入图不一定是最佳的计算图。
*   **原因及对策:**
    *   **特征层面 (Feature Level):**
        *   **输入图缺少特征:**
            *   赋常数值：表达能力受限。
            *   赋唯一 ID (如 one-hot)：计算成本高，难以归纳。
            *   **特征增强 (Feature Augmentation):** 使用图结构信息作为节点特征，如 [[节点度]]、[[聚类系数]]、[[PageRank]]、[[中心性]]、[[GDV]]，或特定结构计数（如节点所在环的长度）。结构感知特征通常计算成本低，易于泛化，且能捕捉 GNN 难以学习的模式。
    *   **结构层面 (Structure Level):**
        *   **图太稀疏:** 消息传递效率低。
            *   添加虚拟边：连接 2-hop 邻居 (`A + A^2`)，对二分图尤其有用。
            *   添加虚拟节点：连接到所有节点，改善稀疏图中的信息传播。
        *   **图太稠密:** 消息传递成本高。
            *   邻居采样：聚合时仅从邻居中随机抽样子集。
        *   **图太大:** 无法放入 GPU。
            *   子图采样：采样一部分节点及其邻域来计算嵌入。 (见 [[扩展 GNN (Scaling Up GNNs)]] )
*   **结论:** 输入图不太可能是最优计算图，需要根据问题进行调整。




### 5 GNN 增强与训练 (GNN Augmentation and Training)

#### 5.1 GNN 预测 (Predictions with GNNs)

*   **获取节点嵌入:** 首先使用 GNN 计算所有相关节点的最终嵌入 `h_v^(L)`。
*   **预测头 (Prediction Heads):** 根据不同任务，在节点嵌入上应用不同的预测层。
    *   **节点级别预测:** 直接使用单个节点嵌入 `h_v^(L)` 进行预测。
        *   **例子:** 线性头 `ŷ_v := Head(h_v^(L)) = W h_v^(L)`。
    *   **边级别预测:** 使用一对节点的嵌入 `(h_u^(L), h_v^(L))` 进行预测。
        *   **例子:**
            *   拼接+线性层: `Head(h_u^(L), h_v^(L)) = W_1 h_u^(L) + W_2 h_v^(L)` 或 `W [h_u^(L) || h_v^(L)]`。
            *   点积: `Head(h_u^(L), h_v^(L)) = (h_u^(L))^T W h_v^(L)` (W 可以是单位阵 I 或可学习矩阵)。
    *   **图级别预测:** 聚合图中所有（或部分）节点的嵌入进行预测。
        *   **例子:** `ŷ_G := Head({h_v^(L) | v ∈ V(G)})`。
        *   **全局池化 (Global Pooling):** 对于小图，可以使用 Mean Pooling, Max Pooling, Sum Pooling。
        *   **分层池化 (Hierarchical Pooling):** 对于大图，全局池化会丢失信息。可以使用 [[DiffPool]] 等方法进行分层聚合。

#### DiffPool (Differentiable Pooling)

*   **解释:** 一种图级别预测的分层池化方法。在每一层：
    *   GNN-A: 计算节点嵌入（嵌入任务）。
    *   GNN-B: 计算节点到下一层聚类的软分配（聚类任务）。
    *   使用 GNN-B 的聚类分配，聚合 GNN-A 的节点嵌入，形成下一层的粗化图节点。
*   **训练:** GNN-A 和 GNN-B 在每一层独立训练（但通常共享部分参数或联合优化）。GNN-A 在第一层基于原始图训练，在后续层基于粗化后的图训练。

#### 5.2 训练图神经网络 (Training Graph Neural Networks)

*   **监督来源 (Ground Truth):**
    *   **监督学习 (Supervised Learning):** 来自外部标签数据。
        *   **例子:** 预测分子的药物活性（标签来自实验）。
    *   **无监督/自监督学习 (Unsupervised/Self-supervised Learning):** 来自图本身的特征。
        *   **例子:** 链接预测（预测图中已存在的边）；图结构重构。
*   **界限模糊:** 有时监督学习和自监督学习的界限不清晰。

#### 5.3 设置 GNN 预测 (Setup GNN Prediction)

*   **数据集划分:** 如何将图数据划分为训练集、验证集、测试集？
    *   **训练集 (Training set):** 用于优化 GNN 参数。
    *   **验证集 (Validation set):** 用于模型选择和超参数调整。
    *   **测试集 (Test set):** 用于报告最终性能。
*   **挑战:** 图数据的划分与图像等独立同分布数据不同。节点之间通过边连接，即使划分了节点，训练集和测试集中的节点也可能通过消息传递相互影响。
*   **两种设置:**
    *   **直推式设置 (Transductive Setting):** 整个图的结构（所有节点和边）在所有划分（训练/验证/测试）中都是可见的。只有训练集的标签是可见的。适用于节点分类、链接预测（当图固定时）。
    *   **归纳式设置 (Inductive Setting):** 训练集、验证集、测试集是不同的图（或通过断开边使原图的不同部分完全独立）。模型必须泛化到未见过的图结构。适用于图分类，或需要处理动态图/新节点的场景。
*   **链接预测的数据划分:**
    *   **自监督任务:** 需要自己生成标签和数据集。
    *   **方法:** 从原始图中隐藏一部分边（作为监督边 `E_sup`），让 GNN 预测这些边是否存在。剩余的边作为消息传递边 `E_msg`。
    *   **划分监督边:** 将 `E_sup` 进一步划分为训练监督边、验证监督边、测试监督边。
        *   **训练:** GNN 看到训练消息边，预测训练监督边。
        *   **验证:** GNN 看到训练消息边 + 训练监督边，预测验证监督边。
        *   **测试:** GNN 看到训练消息边 + 训练监督边 + 验证监督边，预测测试监督边。
    *   **负样本:** 对于链接预测，需要生成负样本（不存在的边）。通常通过随机采样节点对，或破坏正样本边（如替换头/尾节点）来生成。
    *   **Transductive Link Prediction (默认):** 只有一个图，所有节点可见，只划分边。
    *   **Inductive Link Prediction:** 有多个图，或者图在划分时被分割。




### 6 图神经网络理论 (Theory of Graph Neural Networks)

*   **核心问题:** GNN 的表达能力有多强？它们能区分哪些不同的图结构？如何设计表达能力最强的 GNN？

#### GNN 的表达能力与图同构测试

*   **GNN 的区分能力:** GNN 通过聚合邻域信息来区分节点。如果两个节点具有相同的 rooted computation graph（根植于该节点的计算子树结构相同），并且初始特征相同，那么标准的消息传递 GNN 无法区分它们，会给它们生成相同的嵌入。
*   **衡量标准:** GNN 的表达能力可以通过其区分不同计算图（即不同邻域结构）的能力来衡量。最强大的 GNN 应该能将不同的（根植）子树结构映射到不同的嵌入。
*   **目标:** 设计具有**单射邻域聚合 (injective neighborhood aggregation)** 函数的 GNN。
*   **聚合函数对比:** 不同的聚合函数具有不同的区分能力。
    *   `Sum` > `Mean` > `Max` (区分能力递减)。Sum 可以区分多重集 (multiset)，Mean 只能区分分布，Max 只能区分集合元素。
*   **最强大的聚合函数:**
    *   **定理 6.1 (Xu et al. 2019):** 任何单射的多重集函数都可以表示为 `ϕ( Σ(x∈S) f(x) )` 的形式，其中 `f` 和 `ϕ` 是非线性函数（如 MLP）。
    *   **通用近似定理 (Universal Approximation Theorem):** 单隐藏层的 MLP 可以以任意精度近似任何连续函数。
*   **图同构网络 (Graph Isomorphism Network - GIN):**
    *   **设计:** GIN 使用上述 `MLP_ϕ( Σ(x∈S) MLP_f(x) )` 结构作为其聚合函数，旨在达到理论上最强的表达能力。
    *   **GIN 层:**
        `h_v^(l+1) := MLP_ϕ^(l) ( (1 + ϵ^(l)) * h_v^(l) + Σ(u∈N(v)) h_u^(l) )` (原始论文公式，其中 `ϵ` 是可学习或固定参数，MLP 应用于聚合结果)
        或者，等价地：
        `h_v^(l+1) := MLP_ϕ^(l) ( (1 + ϵ^(l)) * MLP_f^(l)(h_v^(l)) + Σ(u∈N(v)) MLP_f^(l)(h_u^(l)) )` (更符合定理形式)
    *   **`ϵ` 的作用:** 允许模型区分中心节点与其邻居节点。
    *   **表达能力:** GIN 的表达能力与 **Weisfeiler-Lehman (WL) 图同构测试** 等价。

#### Weisfeiler-Lehman (WL) Test / Color Refinement

*   **算法:**
    1.  为所有节点分配初始颜色（通常基于节点度或标签）。
    2.  迭代地更新节点颜色：一个节点的新颜色由其自身当前颜色和其邻居颜色（多重集）的哈希值决定。`c^(k+1)(v) := hash({c^(k)(v)} ∪ {c^(k)(u) | u ∈ N(v)})`。
    3.  重复 K 步，最终颜色 `c^(K)(v)` 编码了节点 `v` 的 K-hop 邻域结构。
*   **WL 核 (WL Kernel):** 基于 WL 测试产生的颜色计数来衡量图的相似性。计算效率高 `O(|E|)`。
*   **GIN 与 WL 的关系:** GIN 使用 MLP 来模拟 WL 测试中的哈希函数。`MLP_f` 对应颜色的嵌入，`Σ` 对应收集邻居颜色多重集，`MLP_ϕ` 对应哈希函数。
*   **GIN 相对于 WL 的优势:**
    *   节点嵌入是低维向量，能捕捉节点间细粒度的相似性（WL 颜色是离散的）。
    *   MLP 参数可以针对下游任务进行端到端学习。
*   **局限性:** WL 测试（以及 GIN）无法区分所有非同构图。例如，它无法区分某些规则图（如两个不同度数的非同构正则图）。

#### 6.1 设计最强大的 GNN (GIN)

*   **关键:** 使用能够模拟 WL test 的聚合方案，即单射的多重集函数。
*   **实现:** 通过 `MLP(Σ MLP(·))` 结构实现。

#### 6.2 实践中的问题和调试 (When things don’t go as planned)

*   **数据预处理:** 节点属性需要归一化。
*   **优化器:** Adam 通常比较鲁棒。
*   **激活函数:** ReLU 常用，LeakyReLU/PReLU 可能更好。输出层通常不用激活。每层都要包含偏置项。
*   **嵌入维度:** 32, 64, 128 是不错的起点。
*   **调试:**
    *   **损失/准确率不收敛:** 检查 pipline（梯度清零），调整超参数（学习率），注意权重初始化，仔细检查损失函数。
    *   **模型开发:**
        *   在小数据集上过拟合：损失应能降到接近 0。
        *   监控训练/验证损失曲线：判断过拟合、欠拟合。




### 7 图神经网络的局限性 (Limits of Graph Neural Networks)

*   **理想 GNN 的目标:**
    1.  相同邻域结构 -> 相同嵌入。
    2.  不同邻域结构 -> 不同嵌入。
*   **现有 GNN 的问题:**
    *   **问题 2 (无法区分):** 很多基础 GNN 无法区分一些简单的结构差异（如环的长度）。这是因为它们的表达能力受限于 WL 测试。
    *   **问题 1 (需要区分):** 有时即使节点邻域结构相同，我们也希望根据它们在图中的不同 *位置* 赋予不同的嵌入（位置感知任务 Position-aware tasks）。
*   **目标:** 构建表达能力更强的 GNN 来解决这些问题。

#### GNN 失败的场景 (结构感知任务 Structure-aware tasks)

*   **节点级别:** 环上的节点计算图相同，无法区分（图 7.3）。
*   **边级别:** 预测两条虚线边时，端点的计算图相同，导致预测相同（图 7.4）。
*   **图级别:** 两个不同构的图可能具有完全相同的节点计算图集合（图 7.5），导致图级别预测失败。
*   **根源:** 这些失败与 WL 测试的局限性有关，WL 测试也无法区分这些结构。

#### 7.1 消息传递的光谱视角 (Spectral Perspective of Message Passing)

*   **GIN 的局限性分析:**
    *   GIN 层更新可以写成矩阵形式：`C^(l+1) = MLP( ... Σ(k=0 to 1) A^k C^(l) W_k^(l) ... )` (近似形式)。
    *   使用图邻接矩阵的谱分解 `A = V Λ V^T` (V 是特征向量矩阵，Λ 是特征值对角阵)。
    *   GIN 的更新可以看作是在图拉普拉斯或邻接矩阵的特征向量上进行操作。
    *   节点的最终嵌入依赖于图的特征值和节点初始颜色/特征在特征向量上的投影。
    *   **对称性问题:** 对于图中具有对称性的节点（可以通过图自同构映射相互转换），它们的初始颜色/特征在某些特征向量（与对称性相关的，特别是正交于全1向量的）上的投影可能为零或相同，导致 GNN 无法区分它们。
*   **结论:** 标准消息传递 GNN 的表达能力受限于图的谱特性和对称性。

#### 7.2 特征增强：结构感知的 GNN (Feature-Augmentation: Structurally-Aware GNNs)

*   **目标:** 通过增强节点特征来提升 GNN 的表达能力，使其能区分 WL 测试无法区分的结构。
*   **方法:**
    *   **One-hot 编码:** 为每个节点分配唯一 ID。
        *   优点: 表达能力强。
        *   缺点: 参数量 O(n)，非归纳式，计算成本高。适用于小图和直推式设置。
    *   **常数特征:** 所有节点特征相同。
        *   优点: 简单，易于泛化，低成本，适用于任何图和归纳设置。
        *   缺点: 初始表达能力弱，完全依赖 GNN 从结构中学习。
    *   **结构特征增强:** 使用能够反映节点结构角色的特征。
        *   **例子:** 闭环计数 (Closed loop counts)。使用邻接矩阵幂的对角线 `diag(A^k)` 作为特征，`A^k_{ii}` 表示节点 `i` 上长度为 `k` 的闭环数量。
        *   **C^(0) = [diag(A^0), ..., diag(A^(D-1))]`**
        *   **优点:** 表达能力强（能区分几乎所有真实图），易于泛化，能计数环等结构，计算成本低，适用于任何图。比 WL 核更强大。
        *   **定理 7.1:** 如果两个图的邻接矩阵具有不同的特征值，那么使用闭环特征的 GNN 总能区分它们。

#### 7.3 计数图子结构 (Counting Graph Sub-Structures)

*   **方法:** 使用随机节点 ID/特征来打破对称性，让 GNN 学习计数。
*   **思路:**
    1.  为节点随机分配 ID（来自某个分布）。
    2.  使用 GNN 处理带有随机 ID 的图，得到一个输出 `y`。
    3.  对多次随机分配取期望 `ȳ := E[y]`。
*   **效果:** 期望操作恢复了置换等变性。GNN 在每次采样中可以利用随机 ID 区分原本对称的节点，通过学习聚合这些随机实验的结果，可以学习计数某些子结构。
*   **计算效率:** 通常比谱方法（如特征分解 `O(n^3)` 或 `O(n^2)`）更高效，尤其是当图很大时。

#### 7.4 位置感知的 GNN (Position-Aware GNN)

*   **任务类型:**
    *   **结构感知 (Structure-Aware):** 节点标签由其局部结构角色决定。
    *   **位置感知 (Position-Aware):** 节点标签由其在图中的位置决定。
*   **GNN 的失败:** 标准 GNN 对位置不敏感，因为计算图只依赖于局部邻域结构。
*   **解决方案:** 引入位置编码 (Positional Encoding)，通常作为增强特征。
*   **锚点集方法 (Anchor Sets):**
    *   随机选择一些节点作为“锚点”(anchor nodes)。
    *   将每个节点 `v` 到各锚点 `s_i` 的距离 `d(v, s_i)`（如最短路径距离）作为其位置编码。
    *   **理论基础 (Bourgain 定理):** 随机选择 `O(log^2 n)` 个锚点集，可以将图的度量空间近似嵌入到欧氏空间，保持距离信息。
    *   **P-GNN:** 使用 `O(log^2 n)` 个锚点集，将节点 `v` 嵌入为 `f(v) = [d_min(v, S_{1,1}), ..., d_min(v, S_{log n, c log n})]`，其中 `d_min(v, S) = min_{u∈S} d(v, u)`。
    *   **挑战:** 随机锚点导致编码维度随机排列，需要使用置换不变的神经网络（如 Deep Sets 或特殊设计的 NN 结构）来处理这种位置编码。
    *   **实践:** 训练时每次重新采样锚点集，推理时也采样新的锚点集。

#### 7.5 身份感知的 GNN (Identity-Aware GNNs)

*   **目标:** 解决 GNN 无法区分某些同构计算图的问题（如无法计数环长）。
*   **ID-GNN:** 使用异构消息传递。根据中心节点和邻居节点的相对身份（或颜色）使用不同的消息/聚合函数。
    *   例如，在聚合来自邻居 `s` 的消息时，根据 `s` 是否是中心节点 `v` (`1[s=v]`) 来选择不同的处理方式。
*   **ID-GNN-Fast (简化版):** 将环计数（通过颜色传播或随机 ID 追踪计算得到）作为节点的增强特征，输入到标准 GNN 中。




### 8 图 Transformer (Graph Transformers)

*   **动机:** 将 Transformer 架构（在 NLP 和 CV 中取得巨大成功）应用于图数据。
*   **Transformer 核心组件:**
    1.  **自注意力机制 (Self-Attention Mechanism):** 计算序列中所有元素对之间的相互影响。Query, Key, Value 均来自同一输入序列。
    2.  **编码器-解码器架构 (Encoder-Decoder Architecture):** （在序列到序列任务中）编码器处理输入，解码器生成输出。
    3.  **位置编码 (Positional Encoding):** 由于自注意力本身不感知顺序，需要额外信息来表示位置。
    4.  **多头注意力 (Multi-head Attention):** 并行使用多个注意力机制捕捉不同方面的依赖关系。
    5.  **前馈神经网络 (Feed-forward NN):** 在注意力层之后进行逐点处理。
*   **图 Transformer 的挑战:** 如何将图结构信息融入 Transformer 框架？

#### 8.1 自注意力 (Self-Attention)

*   **机制:**
    1.  每个输入 token `xi` 通过可学习矩阵 `Wq, Wk, Wv` 映射为 Query `qi`, Key `ki`, Value `vi`。
    2.  计算 Query `qi` 和所有 Key `kj` 之间的点积得分 `score(i, j) = qi · kj / sqrt(d)`。
    3.  对得分进行 Softmax 归一化，得到注意力权重 `αi,j`。
    4.  输出 `zi` 是所有 Value `vj` 基于注意力权重的加权和 `zi = Σj αi,j vj`。
*   **矩阵形式:** `Attention(Q, K, V) = softmax(Q K^T / sqrt(d)) V`。
*   **多头注意力:** 并行计算多个头的输出，然后拼接或聚合。

#### 8.2 自注意力与消息传递 (Self-Attention and Message Passing)

*   **等价性:** 自注意力机制可以看作是一种特殊的 GNN 消息传递。
    *   **消息:** `(vj, kj)` 可以看作是从节点 `j` 发出的消息 `Message(Wv xj, Wk xj)`。
    *   **查询:** `q1` 可以看作是节点 1 的查询 `Query(Wq x1)`。
    *   **聚合:** `z1 = Σi softmax_j(qj · kj) vi` 可以看作是节点 1 聚合所有节点的消息 `Aggregate(q1, {Message(xi) | i})`。
*   **关键区别:** 标准自注意力是在一个**全连接图**上进行消息传递，每个节点都与其他所有节点（包括自身）交互。它本身**忽略了图的实际稀疏连接**，并且**对节点顺序不敏感**。

#### 8.3 图 Transformer 的新设计空间 (A New Design Landscape for Graph Transformers)

*   **输入:** 图 Transformer 需要处理 (1) 节点特征，(2) 邻接信息，(3) 边特征。
*   **挑战:** 如何将邻接信息（图结构）和边特征融入 Transformer？标准 Transformer 的输入是 1D 序列。
*   **解决方案:**
    1.  **Tokenization:** 节点特征 `xi` 作为初始的 token 表示。
    2.  **Positional Encoding (关键):** 使用图结构信息来增强 token 表示，使 Transformer 能感知图拓扑。
        *   **基于随机游走的相对距离:** 使用到锚点集的距离作为位置编码（类似 [[P-GNN]]）。对需要计数环等任务有效。
        *   **拉普拉斯特征向量 (Laplacian Eigenvectors):** 使用图拉普拉斯矩阵 `L = D - A` 的特征向量作为位置编码。`L` 捕捉图结构，其特征向量（特别是低频部分）反映全局结构，高频部分反映局部对称性。
            *   **计算:** `L = Σ Λ Σ^T`。使用特征向量矩阵 `Σ` 的列作为节点的 positional encoding。
    3.  **Self-Attention:** 在加入了位置编码的节点表示上执行标准自注意力。
    4.  **边特征嵌入:** 将边特征 `x_ij` 融入注意力得分计算中。例如，修改注意力得分 `a_ij → a_ij + c_ij`，其中 `c_ij` 是基于边特征 `x_ij` 或连接 `i, j` 的路径特征计算得到的值（如 `w_e^T x_ij`）。

#### 8.4 图 Transformer 的位置编码 (Positional Encodings for Graph Transformers)

*   **拉普拉斯特征向量的问题:**
    *   **符号模糊性 (Sign Ambiguity):** 如果 `v` 是特征向量，则 `-v` 也是。特征分解算法可能随机输出 `v` 或 `-v`，导致 GNN 预测不稳定。
*   **解决方案:**
    *   **数据增强:** 训练时随机翻转特征向量符号。缺点是指数组合多。
    *   **设计符号不变的网络层 (SignNet):**
        *   **定理 8.1:** 函数 `f(x)` 满足 `f(x) = f(-x)` (符号不变) 当且仅当存在函数 `g` 使得 `f(x) = g(x) + g(-x)`。
        *   **应用:** 设计神经网络层结构为 `Aggregate(g(xi) + g(-xi))`。
*   **完整流程:**
    1.  计算拉普拉斯特征向量 `Σ`。
    2.  使用 SignNet 处理 `Σ` 得到符号不变的特征向量嵌入 `pi`。
    3.  将 `pi` 与原始节点特征 `xi` 拼接。
    4.  将拼接后的特征输入主 GNN/Transformer 模型。
    5.  （训练时）反向传播梯度，联合训练 SignNet 和主模型。




### 9 异构图机器学习 (Machine Learning with Heterogeneous Graphs)

*   **定义:** 异构图 (Heterogeneous Graph) 是节点或边包含多种类型的图。`G = (V, E, τ, ϕ)`，`τ` 是节点类型映射，`ϕ` 是边类型映射。
*   **关系类型 (Relation Type):** 由边的源节点类型、边类型、目标节点类型定义的三元组 `r = (τ(u), ϕ(u, v), τ(v))`。
*   **r-邻域 (r-neighborhood):** `Nr(v) = {u ∈ N(v) | ϕ(u, v) = r}`。
*   **为什么需要异构图:**
    *   不同类型的节点/边可能具有不同维度或含义的特征。
    *   不同类型的关系代表不同的交互模式。
*   **处理方式:** 可以将类型信息作为节点/边特征编码到同构图中，但这可能丢失信息。异构图模型旨在直接处理多样的类型。

#### 9.2 关系型 GCN (Relational GCN - R-GCN)

*   **目标:** 将 GCN 扩展到异构图。
*   **核心思想:** 为每种关系类型 `r ∈ R` 使用不同的权重矩阵 `W_r^(l)` 进行消息传递。
*   **R-GCN 层 (有向图为例):**
    `h_v^(l+1) := σ( Σ_{r∈R} Σ_{u∈Nr(v)} (1/c_{v,r}) * W_r^(l) * h_u^(l) + W_0^(l) * h_v^(l) )`
    *   `W_r^(l)`: 关系 `r` 的变换矩阵。
    *   `W_0^(l)`: 自环的变换矩阵。
    *   `c_{v,r}`: 归一化常数 (如 `|Nr(v)|`)。
*   **消息-聚合形式:**
    *   消息 (邻居): `m_{u,r}^(l) = (1/c_{v,r}) W_r^(l) h_u^(l)`
    *   消息 (自身): `m_v^(l) = W_0^(l) h_v^(l)`
    *   聚合: `h_v^(l+1) = σ( Σ_{r∈R} Σ_{u∈Nr(v)} m_{u,r}^(l) + m_v^(l) )` (聚合方式可以是求和)
*   **参数量问题:** 如果关系类型 `|R|` 很多，参数量 (`|R| * d^(l+1) * d^(l)`) 会非常大，容易过拟合。
*   **正则化方法:**
    *   **基分解/字典学习 (Basis/Dictionary Learning):** 将每个 `W_r` 表示为一组共享基矩阵 `V_b` 的线性组合：`W_r = Σ_{b=1}^B a_{r,b} V_b`。需要学习的参数是系数 `a_{r,b}` 和基矩阵 `V_b`。参数量减少为 `|R|*B + B*d^(l+1)*d^(l)`。
    *   **块对角矩阵 (Block Diagonal Matrices):** 将每个 `W_r` 限制为块对角形式。每个块内进行密集连接，块间无连接。减少参数量，但限制了特征维度间的交互。
*   **异构图任务:**
    *   **节点标签预测:** 同 GCN。
    *   **链接预测 (关系特定):** 预测特定关系 `r` 的边 `(u, v)` 是否存在。可以使用关系特定的评分函数 `f_r(h_u, h_v) = h_u^T W_r h_v` (DistMult 形式) 或其他。需要生成负样本。

#### 9.3 异构图 Transformer (Heterogeneous Graph Transformer - HGT)

*   **目标:** 将 Transformer 的注意力机制应用于异构图。
*   **挑战:** 直接为每种关系类型引入独立注意力参数会导致参数爆炸 (`|T|^2 * |R|` 种交互)。
*   **异构相互注意力 (Heterogeneous Mutual Attention):** 将注意力计算分解为与节点类型、边类型相关的组件。
    *   **注意力计算:** `α_{v,u} ∝ k_{τ(u)}(h_u)^T W_{ϕ(u->v)}^{att} q_{τ(v)}(h_v)` (二次型形式)。`k`, `q` 是节点类型特定的投影，`W^{att}` 是边类型特定的注意力矩阵。
    *   **消息计算:** `m_u = W_{ϕ(u->v)}^{msg} N_{τ(u)} h_u`。`N` 是节点类型特定的线性层，`W^{msg}` 是边类型特定的消息变换矩阵。
*   **HGT 层:**
    `h_v^(l) := Aggregate( { softmax_u(α_{v,u}) · m_u^(l) | u ∈ N(v) } )` (聚合可以考虑多头)
*   **优点:** 参数量相对 R-GCN 较少，性能通常更好（尤其在 OGB-MAG 数据集上）。

#### 9.4 异构 GNN 设计空间 (Design Space for Heterogeneous GNNs)

*   **异构消息聚合 (Heterogeneous Message Aggregation):** 可以为不同的关系类型 `r` 使用不同的聚合函数 `Agg_r^(l)`。
    `h_v^(l+1) := Agg_{outer}( { Agg_r^(l)({m_u^(l) | u ∈ N_r(v)}) | r ∈ R } )`
    *   **常见选择:** 内部聚合 `Agg_r` 为求和，外部聚合 `Agg_{outer}` 为拼接。




### 10 知识图谱嵌入 (Knowledge Graph Embeddings)

*   **知识图谱 (Knowledge Graphs - KG):** 异构图，节点是实体 (entities)，边表示实体间的关系 (relations)。通常表示为三元组 (头实体 h, 关系 r, 尾实体 t) 或 (h, r, t)。
*   **特点:**
    *   **大规模:** 数百万节点和边。
    *   **不完整:** 许多真实存在的关系缺失。
*   **目标:** 学习实体和关系的低维嵌入，用于知识图谱补全 (KG Completion) 等任务。

#### 10.1 知识图谱补全 (Knowledge Graph Completion)

*   **任务:** 给定 (h, r)，预测可能的尾实体 t；或给定 (r, t)，预测头实体 h。与链接预测类似，但更侧重于推理和补全缺失的事实。
*   **核心思想 (浅层嵌入):** 为每个实体 `e` 和关系 `r` 学习嵌入向量 `e, r`。定义一个评分函数 `score(h, r, t)`，使得存在的三元组得分高，不存在的得分低。
*   **设计问题:**
    1.  如何嵌入实体和关系？
    2.  如何定义评分函数（衡量接近度）？
*   **关系模式:** KG 中的关系可能具有不同属性，嵌入模型需要能捕捉这些模式：
    *   **对称性 (Symmetry):** `r(h, t) ⇒ r(t, h)` (如 `is_roommate_of`)
    *   **反对称性 (Antisymmetry):** `r(h, t) ⇒ ¬r(t, h)` (如 `is_father_of`)
    *   **逆关系 (Inverse):** `r1(h, t) ⇒ r2(t, h)` (如 `advisor/advisee`)
    *   **组合/传递性 (Composition/Transitivity):** `r1(x, y) ∧ r2(y, z) ⇒ r3(x, z)` (如 `location/contains`)
    *   **1-to-N, N-to-1, N-to-N 关系:** (如 `student_of`)
*   **嵌入模型 (Table 10.1):**
    *   **TransE:**
        *   **思想:** 将关系 `r` 视为头实体 `h` 到尾实体 `t` 的翻译向量，即 `h + r ≈ t`。
        *   **评分:** `score(h, r, t) = - ||h + r - t||` (距离越小越好)。
        *   **嵌入:** `h, r, t ∈ R^k`。
        *   **能力:** 能处理逆关系和组合关系，但难以处理对称和 1-to-N 等关系。
    *   **TransR:**
        *   **思想:** 实体在不同的关系下有不同的表示。引入关系特定的投影矩阵 `Mr ∈ R^(d×k)`，将实体嵌入 `h, t ∈ R^k` 投影到关系空间 `R^d` 中，再应用 TransE 的思想：`Mr h + r ≈ Mr t`。
        *   **评分:** `score(h, r, t) = - ||Mr h + r - Mr t||`。
        *   **嵌入:** `h, t ∈ R^k`, `r ∈ R^d`, `Mr ∈ R^(d×k)`。
        *   **能力:** 比 TransE 更灵活，能处理 1-to-N 等关系。
    *   **DistMult:**
        *   **思想:** 使用双线性评分函数。
        *   **评分:** `score(h, r, t) = <h, r, t> = Σi hi ri ti`。
        *   **嵌入:** `h, r, t ∈ R^k`。
        *   **能力:** 只能处理对称关系。
    *   **ComplEx:**
        *   **思想:** 将嵌入扩展到复数域 `C^k`。
        *   **评分:** `score(h, r, t) = Re(<h, r, t̄>)` (t̄ 是 t 的共轭)。
        *   **嵌入:** `h, r, t ∈ C^k`。
        *   **能力:** 能有效处理对称和反对称关系。
    *   **RotatE:**
        *   **思想:** 将关系 `r` 建模为复数空间中的旋转。`t = h ◦ r`，其中 `◦` 是元素级别的 Hadamard 积，且 `|ri| = 1`。
        *   **评分:** `score(h, r, t) = - ||h ◦ r - t||`。
        *   **嵌入:** `h, t ∈ C^k`, `r ∈ C^k` 且 `|ri|=1`。
        *   **能力:** 能同时处理对称、反对称、逆、组合关系。
*   **训练 (以 TransE 为例 - Algorithm 10.1):**
    *   使用负采样。对于每个正三元组 `(h, r, t)`，构造一个负三元组 `(h', r, t')` (通过替换 `h` 或 `t`)。
    *   使用 **Margin Loss (对比损失):** `L = Σ [(γ + d(h+r, t) - d(h'+r', t'))]+`。目标是让正样本的距离（不相似度）加上一个 margin `γ` 后，仍然小于负样本的距离。`d(x, y) = ||x - y||`。
    *   使用 SGD 更新实体和关系嵌入。
*   **选择:** 没有 universally 最好的模型，需要根据 KG 的特性和任务选择。




### 11 知识图谱推理 (Reasoning in Knowledge Graphs)

*   **目标:** 回答涉及多跳路径和逻辑运算（如合取 `∧`）的复杂查询。
*   **挑战:** KG 不完整，简单图遍历可能失败。
*   **多跳推理 (Multi-Hop Reasoning):**
    *   **1-hop 查询:** `(h, r)` -> `?t` (等价于 KG 补全)。
    *   **n-hop 路径查询:** `q = (va, (r1, ..., rn))` -> `?t`。查询从锚点实体 `va` 开始，沿着关系路径 `r1, ..., rn` 进行推理。
    *   **合取查询 (Conjunctive Queries):** 涉及多个子查询和交集运算 `∩`。例如，“找到既治疗乳腺癌又导致头痛的药物？” `q = q1 ∧ q2`，答案是 `llbracket q \rrbracket_G = \llbracket q1 \rrbracket_G ∩ \llbracket q2 \rrbracket_G`。

#### 11.1 在知识图谱上回答预测性查询 (Answering Predictive Queries on Knowledge Graphs)

*   **基于嵌入的推理:**
    *   **路径查询 (TransE):** 将路径查询嵌入为向量 `q = va + r1 + ... + rn`。在嵌入空间中查找靠近 `q` 的实体嵌入作为答案。TransE 无法处理有序路径。
    *   **合取查询 (挑战):** 如何在嵌入空间中表示实体集合（中间查询结果）以及如何执行交集运算？

#### 11.2 Query2Box

*   **核心思想:** 将实体和查询都嵌入为高维空间中的**轴对齐超矩形 (Boxes)**。
*   **Box 表示:** 每个 Box 由一个中心 (center) 向量 `c` 和一个偏移 (offset) 向量 `o` (>=0) 定义。Box 包含所有点 `p` 使得 `c - o ≤ p ≤ c + o` (逐元素)。
*   **优点:** Box 的交集仍然是 Box（或空集），便于进行交集运算。
*   **嵌入:**
    *   **实体嵌入:** 视为零体积的 Box (offset = 0)。
    *   **关系嵌入 (投影算子 P):** 每个关系 `r` 学习一个投影算子，将输入的 Box `q` 变换为一个新的 Box `q' = P(q, r)`。`center(q') = center(q) + center(r)`, `offset(q') = offset(q) + offset(r)`。关系嵌入 `r` 也被建模为 Box (center, offset)。
    *   **交集算子 I:** 输入多个 Box `{qi}`，输出它们的（近似）交集 Box `q∩`。
        *   `center(q∩) = Σ_i w_i * center(qi)` (加权平均，权重 `wi` 通过 MLP 基于中心计算，表示注意力)。
        *   `offset(q∩) = min({offset(q1), ..., offset(qn)}) ⊙ σ(MLP({offset(q1), ..., offset(qn)}))` (偏移量取最小值以保证交集缩小，并通过 Sigmoid 调制)。
*   **回答查询:** 将查询结构（如查询计划图）转换为嵌入空间中的 Box 操作序列（投影和交集），最终得到答案 Box。
*   **评分函数:** `fq(v)` 衡量实体 `v` (点) 与答案 Box `q` 的相关性。
    *   `dbox(q, v) = dout(q, v) + α * din(q, v)`
    *   `dout`: 实体 `v` 到 Box 外部的距离。
    *   `din`: 实体 `v` 到 Box 中心的距离（在 Box 内部）。
    *   `α` (0<α<1): 缩小内部距离的权重。
    *   `fq(v) = -dbox(q, v)` (距离越小，分数越高)。使用曼哈顿 (L1) 距离更自然。

#### 11.3 训练 Query2Box

*   **目标:** 对于查询 `q`，最大化正答案 `v ∈llbracket q \rrbracket` 的得分 `fq(v)`，最小化负答案 `v' ∉ \llbracket q \rrbracket` 的得分 `fq(v')`。
*   **损失函数:** 类似于 KG 补全的损失，对每个查询 `q` 及其正负样本 `v, v'`，最小化 Log Sigmoid 损失：
    `L = E [- log σ(fq(v)) - log(1 - σ(fq(v')))]` (KL 散度 / Logistic Loss 形式)。
*   **训练:** 从训练 KG 中采样查询、正答案、负答案，通过反向传播优化实体嵌入、关系嵌入（投影算子）、交集算子中的 MLP 参数。
*   **处理复杂查询 (析取 `∨`):**
    *   **析取范式 (Disjunctive Normal Form - DNF):** 任何 EPFO 查询都可以转换为 `q = q1 ∨ ... ∨ qn` 的形式，其中每个 `qi` 是一个合取查询（对应一个 Box）。
    *   **距离计算:** 实体 `v` 到 DNF 查询 `q` 的距离定义为它到所有子查询 Box `qi` 的最小距离：`dbox(q, v) = min_i {dbox(qi, v)}`。
    *   **实现:** 将联合操作留到最后一步的距离计算中。




### 12 GNN 用于推荐系统 (GNNs for Recommender Systems)

*   **背景:** 推荐系统旨在预测用户可能感兴趣的物品。
*   **数据建模:** 用户-物品交互数据可以自然地建模为**二分图 (Bipartite Graph)**。
    *   节点: 用户 `U` 和物品 `V`。
    *   边 `E`: 连接用户和物品，表示交互（如购买、点击、评分），通常带有时间戳。
*   **任务:** 链接预测。给定过去交互，预测未来用户会与哪些新物品交互。
*   **挑战:** 物品数量巨大 (|V| 可能达数十亿)，对所有 (用户, 物品) 对评分不可行。
*   **两阶段推荐流程:**
    1.  **候选生成 (Candidate Generation):** (快速、廉价) 从海量物品中召回少量（如 1000 个）候选物品。通常基于嵌入。
    2.  **排序 (Ranking):** (慢速、精确) 对候选物品使用更复杂的模型（如 GNN）进行评分 `f(u, v)` 并排序，推荐 Top K。
*   **评估指标:** Recall@K = `|Pu ∩ Ru| / |Pu|`。`Pu` 是用户未来实际交互的正样本集合，`Ru` 是推荐的 Top K 物品集合。目标是最大化 Recall@K。

#### 12.1 推荐系统：基于嵌入的模型 (Recommender Systems: Embedding Based Models)

*   **核心思想:** 为用户 `u` 和物品 `v` 学习嵌入向量 `u, v ∈ R^D`，评分函数 `f(u, v)` 通常基于嵌入计算（如点积 `u · v`）。
*   **训练目标:** 优化嵌入以提高 Recall@K。由于 Recall@K 不可微，使用代理损失函数 (Surrogate Loss Functions)。
*   **代理损失函数:**
    *   **二元损失 (Binary Loss):** 将交互视为二分类问题（交互=1，未交互=0）。
        *   正样本: 观测到的边 `E`。
        *   负样本: 未观测到的边 `E-` (所有可能的 (u, v) 对减去 E)。
        *   损失: `- (1/|E|) Σ_{(u,v)∈E} log σ(fθ(u, v)) - (1/|E-|) Σ_{(u,v)∈E-} log(1 - σ(fθ(u, v)))`。
        *   **问题:** 负样本数量巨大，需要采样；可能不必要地惩罚模型（用户间评分无关）。训练困难。
    *   **贝叶斯个性化排序 (Bayesian Personalized Ranking - BPR):**
        *   **目标:** 对每个用户 `u*`，希望其交互过的物品 `v+` 的得分高于未交互过的物品 `v-` 的得分。
        *   ** rooted 正/负样本:** `E(u*)` 和 `E-(u*)`。
        *   **损失 (单个用户):** `L(u*) ∝ Σ_{(u*,v+)∈E(u*)} Σ_{(u*,v-)∈E-(u*)} - log σ(fθ(u*, v+) - fθ(u*, v-))`。
        *   **总体损失:** `L = (1/|U|) Σ_{u*∈U} L(u*)`。
        *   **训练:** Mini-batch 训练，采样用户 `Û`，对每个 `u* ∈ Û` 采样一个正样本 `v+` 和多个负样本 `V-`。
        *   **优点:** 更关注用户内部的排序，优化目标更接近推荐任务。需要采样大量负样本。
*   **协同过滤 (Collaborative Filtering):** 嵌入模型能工作的核心原因是协同过滤效应。用户的偏好与许多其他相似用户的偏好相关。低维嵌入迫使模型学习用户和物品间的相似性。

#### 12.2 神经图协同过滤 (Neural Graph Collaborative Filtering - NGCF)

*   **动机:** 传统协同过滤（如矩阵分解、浅层嵌入）只隐式捕捉图结构（通过损失函数），没有显式利用图上的高阶连接信息。
*   **核心思想:** 使用 GNN 在用户-物品二分图上传播信息，生成显式包含图结构信息的嵌入。
*   **方法:**
    1.  初始化用户 `u` 和物品 `v` 的嵌入 `h_u^(0), h_v^(0)` (可以是随机或预训练的浅层嵌入)。
    2.  通过 K 层 GNN 迭代更新嵌入：
        `h_v^(k+1) = Combine(h_v^(k), Aggregate({h_u^(k) | u ∈ N(v)}))` (物品更新)
        `h_u^(k+1) = Combine(h_u^(k), Aggregate({h_v^(k) | v ∈ N(u)}))` (用户更新)
    3.  最终嵌入为 `u ← h_u^(K)`, `v ← h_v^(K)`。
    4.  评分函数为点积 `u^T v`。
*   **优点:** 显式建模高阶连通性。

#### 12.3 LightGCN

*   **动机:** NGCF 引入了 GNN 参数（`Wk, Bk` 等），但用户/物品嵌入本身已经非常具有表达力 (参数量 O(ND))，GNN 参数可能不是必需的 (参数量 O(D^2))。
*   **核心思想:** 简化 NGCF，移除 GNN 中的特征变换 (`Wk`) 和非线性激活 (`σ`)。只保留邻域聚合（信息传播）。
*   **方法:**
    *   令 `E^(0)` 为初始嵌入矩阵 ([`EU`; `EV`])。
    *   GCN 层简化为 `E^(k+1) = Ã E^(k)`，其中 `Ã = D^(-1/2) A D^(-1/2)` 是归一化的邻接矩阵（无自环）。
    *   多次迭代 `E ← Ã E` 进行 K 次信息传播。`E^(K) = Ã^K E^(0)`。
    *   最终嵌入是各层嵌入的加权平均（多尺度扩散）：`E = Σ_{k=0}^K α_k E^(k) = Σ_{k=0}^K α_k Ã^k E^(0)`。LightGCN 使用 `α_k = 1/(K+1)`。
*   **优点:** 模型更简单，参数更少，性能通常优于 NGCF。与 Correct & Smooth (C&S) 类似，但用于推荐。
*   **计算:** `Ã^K` 是稠密矩阵，不直接计算。而是迭代执行 K 次稀疏矩阵-稠密矩阵乘法 `E ← Ã E`。

#### 12.4 PinSAGE

*   **背景:** Pinterest 使用的工业级 GNN 推荐系统，用于推荐 Pin (图片)。
*   **目标:** 学习 Pin 的嵌入 `zi`，使得语义相似的 Pin 嵌入距离近。统一了视觉、文本、图信息。
*   **挑战:** 图规模巨大（百亿节点、千亿边），需要高效扩展 GNN。
*   **扩展方法:**
    *   **跨用户共享负样本:** 在 BPR 损失的 mini-batch 训练中，所有采样用户共享同一组负样本物品，减少计算图数量，降低计算成本。
    *   **困难负样本采样 (Hard Negative Sampling):** 随机负样本太“简单”，模型无法学到细粒度的区分。需要采样与查询 Pin 相似但不相关的困难负样本。
        1.  基于性化 PageRank (PPR) 对物品排序。PPR 分数高的物品与用户相关的可能性大。
        2.  从 PPR 排名较高但不是过高（如 2000-5000 名）的物品中随机采样作为困难负样本。
    *   **课程学习 (Curriculum Learning):** 训练过程中逐步增加困难负样本的比例。
    *   **GNN 的 Mini-batch 训练:** (将在 [[扩展 GNN]] 章节详述) 如邻居采样。




### 13 关系型深度学习 (Relational Deep Learning)

*   **背景:** 大量数据存储在关系型数据库 (Relational Databases) 中。
*   **目标:** 将深度学习（特别是 GNN）应用于关系型数据库，进行端到端的预测任务，避免传统方法中的手动特征工程和阻抗失配 (impedance mismatch) 问题。
*   **阻抗失配:** 使用简单模型（如 SQL+简单 ML）处理复杂关系数据时遇到的问题：特征选择随意、数据使用受限、难以保证时间点正确性/信息泄漏。
*   **端到端学习优势:** 模型更准确（无特征工程损失）、更鲁棒（特征自动更新）、建模时间更短、基础设施更简单。
*   **关系型深度学习 (RDL):** 旨在结合关系数据库和深度学习，是统计关系学习 (SRL) 的继承者。

#### 13.1 关系数据库图 (Relational Database Graph)

*   **转换:** 将关系数据库表示为一个图。
*   **数据库模式:** 一组表 `T = {T1, ..., Tn}` 和表之间的连接（外键关系）`L ⊆ Ti × Tj`。
*   **模式图 (Schema Graph):** 表示表和它们之间允许的连接。
*   **关系实体图 (Relational Entity Graph):**
    *   **节点 V:** 数据库中所有表的所有行（实体）的并集。
    *   **边 E:** 如果实体 `u` 的主键出现在实体 `v` 的外键中（或反之），则在 `u` 和 `v` 之间添加边。
    *   **特征:** 每个实体（节点）可以拥有其在表中对应的行属性作为特征。
*   **与知识图谱的区别:** 实体具有丰富的数值/类别特征。

#### 13.2 关系数据库中的预测任务 (Predictive Tasks in Relational Databases)

*   **特点:** 任务通常是**时态的 (temporal)**，因为数据库随时间变化，实体的标签也可能随时间变化。
*   **设置 (以用户流失预测为例):**
    *   **任务:** 预测用户是否会在未来 30 天内流失。
    *   **训练表 (Training Table):** 包含 `(entityId, time, label)` 列。一个实体在不同时间点可能有不同标签。例如 `(user, time, churn)`。
    *   **时间依赖性:** 预测 `t` 时刻的标签，只能使用 `t` 时刻之前的数据。
*   **GNN 应用:**
    *   将训练表附加到关系实体图上。
    *   每个节点的计算图是其邻域，且这个计算图是**时间依赖**的。消息传递和聚合过程需要考虑时间戳，只聚合来自过去的交互信息。

#### SQL Join 与 GNN Aggregation

*   **Excursion: SQL Join:** 关系数据库中的 Join 操作（如 `R ⨝ S`）基于共同的键 (key) 合并两个表中的行，本质上是笛卡尔积的子集。
*   **GNN 的能力:** GNN 的聚合 (Aggregation) 和消息传递 (Message Passing) 机制可以隐式地学习和执行类似 SQL Join 和 Aggregation 的操作，从而在多个相关表中进行多跳推理和模式发现。

#### 13.3 RelBench

*   **平台:** 一个用于评估 GNN 在关系数据库任务上性能的基准测试集合。
*   **比较:** 将端到端 GNN 方法与传统数据科学工作流（EDA、特征工程、SQL 查询、XGBoost 模型、SHAP 分析）进行比较。
*   **结果:** GNN 在 RelBench 的多个任务上持续优于人类专家（数据科学家）设计的工作流，显示了端到端关系深度学习的潜力。




### 14 快速神经子图匹配与计数 (Fast Neural Subgraph Matching and Counting)

*   **子图 (Subgraphs):** 网络的构建模块，具有区分和表征网络的能力。
*   **例子:** 有机分子中的官能团。

#### 14.1 子图和模体 (Subgraphs and Motifs)

*   **子图类型:**
    *   **节点诱导子图 (Node-induced subgraph):** 由节点子集 V' 及 V' 内部的所有边构成。
    *   **边诱导子图 (Edge-induced subgraph):** 由边子集 E' 及 E' 关联的所有节点构成。
*   **图同构 (Graph Isomorphism):** 判断两个图拓扑结构是否完全相同。计算上未知是否 NP-hard，但没有已知的多项式时间算法。
*   **子图同构 (Subgraph Isomorphism):** 判断图 G1 是否同构于图 G2 的某个（诱导）子图。NP-hard 问题。
*   **网络模体 (Network Motif):** 在网络中**反复出现的**、**显著的**连接模式（通常是小的节点诱导子图）。
    *   **反复出现 (Recurring):** 出现次数多。需要定义频率。
    *   **显著 (Significant):** 出现频率远高于在某种随机图模型（零模型）中期望的频率。
*   **频率定义:**
    *   **图级频率 (Graph-level Frequency):** 查询图 `GQ` 在目标图 `GT` 中作为（节点）诱导子图出现的次数（不同节点集的诱导子图算多次）。
    *   **节点级频率 (Node-level Frequency):** 固定查询图 `GQ` 中的一个锚点 `v`，计算目标图 `GT` 中有多少个节点 `u`，使得 `u` 所在的某个 `GQ` 同构子图中，`u` 对应于锚点 `v`。
*   **显著性评估:**
    *   **零模型 (Null Model):** 生成与真实网络具有某些相同属性（如节点数、边数、度分布）的随机图。
        *   **ER 随机图 (Erdős-Rényi Random Graphs):** `G(n, p)`，边独立随机出现。度分布为二项分布，不符合真实网络。
        *   **配置模型 (Configuration Model):** 保持与真实图完全相同的度序列。将节点的度看作“插口”，随机连接插口。
        *   **切换模型 (Switching Model):** 从真实图出发，随机选择两条边 `(a, b), (c, d)`，交换端点变为 `(a, d), (c, b)`（若不产生重边或自环），重复多次。保持度序列，逐渐随机化结构。最慢。
    *   **Z-score:** `Zi = (N_real(i) - mean(N_rand(i))) / std(N_rand(i))`。衡量模体 `i` 在真实图中出现的次数相对于随机图的均值和标准差。
    *   **网络显著性谱 (Network Significance Profile - SP):** 归一化的 Z-score 向量 `SPi = Zi / sqrt(Σ_j Zj^2)`。用于比较不同网络的模体模式。来自同一领域的网络通常具有相似的 SP。

#### 14.2 神经子图表示 (Neural Subgraph Representations)

*   **目标:** 使用 GNN 进行子图匹配（判断 `GQ` 是否是 `GT` 的子图）。
*   **核心思想:** 利用嵌入空间的几何结构来表示子图关系。
*   **方法 (基于锚点节点级频率):**
    1.  **分解:** 将目标图 `GT` 分解为以每个节点 `v` 为锚点的 K-hop 邻域 `N_k(v)`。
    2.  **嵌入:** 使用 GNN 将查询图 `GQ`（以某个节点为锚点）和目标图的邻域 `N_k(v)` 嵌入到同一个向量空间。令嵌入分别为 `zQ` 和 `zN(v)`。
    3.  **序约束 (Order Constraint):** 设计 GNN 和嵌入空间，使得子图同构关系 `GQ ⊆ N_k(v)` 对应于嵌入空间中的偏序关系 `zQ ≤ zN(v)` (逐元素小于等于)。要求嵌入向量非负。
    4.  **匹配:** 如果 `zQ ≤ zN(v)` 成立，则判定 `GQ` 是 `N_k(v)` 的子图。
*   **损失函数 (Max-margin Loss):**
    *   **目标:** 学习满足序约束的嵌入。
    *   **违反度:** `E(Gq, Gt) = Σ_i max(0, zq[i] - zt[i])^2`。如果 `Gq ⊆ Gt`，希望 `E=0`。
    *   **训练样本:** 生成正样本对 (`Gq ⊆ Gt`) 和负样本对 (`Gq <binary data, 1 bytes><binary data, 1 bytes> Gt`)。
    *   **损失:** `L(Gq, Gt) = E(Gq, Gt)` (对于正样本)，`L(Gq, Gt) = max(0, α - E(Gq, Gt))` (对于负样本)。`α` 是 margin。负样本的损失在违反度小于 margin 时为 0，防止嵌入被推得无限远。
*   **样本生成:**
    *   **正样本:** 从 `GT` 中通过 BFS 采样诱导子图 `GQ`。
    *   **负样本:** 对正样本 `GQ` 进行微小的修改（增/删节点/边）使其不再是子图。
*   **推理:** 计算 `zQ` 和 `zN(v)`，判断 `E(GQ, N_k(v)) < ε` 是否成立。

#### 14.3 挖掘频繁模体 (Mining Frequent Motifs)

*   **目标:** 给定大小 `k`，在目标图 `GT` 中找到出现频率最高的 `r` 个（或所有）大小为 `k` 的模体。
*   **挑战:** 枚举所有大小 `k` 的子图并计数非常困难。
*   **SPMiner 算法 (基于神经子图表示):**
    1.  **预计算/嵌入:** 将 `GT` 分解为锚定邻域，并使用预训练的 GNN（满足序约束）将它们嵌入到序嵌入空间。
    2.  **频率估计:** 对于任何候选模体 `GQ`，可以通过计算有多少个目标邻域嵌入 `zN` 满足 `zQ ≤ zN` 来快速估计其频率 `|SE|`。`SE = {zQ | zQ ≤ zN, GN ⊆ GT}`。
    3.  **模体游走 (Motif Walk - Beam Search):**
        *   从 `GT` 中随机选择一个起始节点 `u`，初始化当前模体 `S = {u}`。
        *   迭代地增长模体 `S`：选择 `S` 中节点的邻居 `v` 加入 `S`，使得新模体 `S ∪ {v}` 的**估计频率 `|SE|` 最大化**（或等价地，**总违反度 `Σ_{zN} E(S∪{v}, zN)` 最小化**）。这是一个贪心策略。
        *   重复直到达到目标模体大小 `k`。
        *   执行多次模体游走以探索不同的模体。
*   **优点:** 避免了显式的子图同构测试，利用 GNN 学习的嵌入空间进行高效搜索。




### 15 标签传播 (Label Propagation)

*   **背景:** 在只有部分节点有标签的图上进行节点分类（半监督学习）。
*   **核心思想:** 利用网络中的相关性（连接的节点倾向于有相同标签）将已知标签传播到未知标签的节点。基于 **同质性 (Homophily)** 和 **影响力 (Influence)** 的假设。
    *   **同质性:** 物以类聚，相似的节点倾向于连接。
    *   **影响力:** 节点可以影响其邻居的属性/标签。
*   **方法:**

#### 15.1 标签传播算法 (Label Propagation - LP)

*   **算法:**
    1.  初始化：有标签节点的预测标签 `Ŷv = Yv` (one-hot 或概率)，无标签节点的预测标签 `Ŷv` 初始化为均匀分布（或 0.5）。
    2.  迭代更新无标签节点的标签概率分布：节点 `v` 的新标签分布是其邻居标签分布的加权平均。
        `P^(t+1)(Yv = c) = Σ_{(u,v)∈E} (A_{vu} / Σ_k A_{vk}) * P^(t)(Yu = c)` (归一化形式)
        或者使用非归一化形式：
        `Ŷv^(t+1) = Σ_{u∈N(v)} A_{vu} Ŷu^(t)` (需要后续归一化)
    3.  重复直到收敛或达到最大迭代次数。
*   **特点:** 简单、快速（每次迭代）。
*   **问题:**
    *   收敛速度可能很慢，且不保证收敛（如在二分图上）。
    *   不使用节点属性特征。

#### 15.2 Correct and Smooth (C&S)

*   **动机:** GNN 在节点特征信息不足或标签与特征相关性弱时表现不佳。C&S 旨在结合 GNN（或其他基础预测器）的预测和标签传播的平滑性。
*   **核心思想:** 基础预测器的**误差**应该在图上是平滑的（相邻节点的预测误差相似）。
*   **步骤:**
    1.  **训练基础预测器:** 使用 GNN 或其他模型，在有标签的节点上训练，得到对所有节点（包括无标签节点）的软标签预测 `P̂`。
    2.  **Correct (修正):**
        *   计算训练集上的误差 `Ev = Pv - P̂v` (Pv 是真实标签 one-hot)。无标签节点误差为 0。
        *   在图上传播/扩散误差 `E`，得到平滑后的误差 `Ẽ`。
            `E^(t+1) = (1 - α) E^(t) + α Ã E^(t)` (Ã 是归一化邻接矩阵，如 `D^(-1/2)AD^(-1/2)`)
        *   将修正后的误差加回到基础预测上：`P̃ = P̂ + s Ẽ` (s 是缩放因子)。
    3.  **Smooth (平滑):**
        *   将 Correct 步骤得到的修正预测 `P̃`（视为初始标签分布）在图上进行标签传播/扩散。
        *   `Z^(t+1) = (1 - α) Z^(t) + α Ã Z^(t)`，初始化 `Z^(0) = P̃`。
        *   最终预测是平滑后的结果 `Z^(T)`。
*   **扩散矩阵:** `Ã` 决定了信息如何在图上传播。
*   **优点:** 模型无关（可用于任何基础预测器），显著提升基础模型性能，表现优于仅平滑基线，可与 GNN 结合。`α` 控制同质性程度。

#### 15.3 掩码标签预测 (Masked Label Prediction)

*   **思想:** 类似于 NLP 中的 BERT。将节点标签视为额外的节点特征。
*   **方法:**
    1.  将节点特征 `X` 和部分可见的标签 `Ẏ` (来自训练集) 拼接成增广特征 `[X, Ẏ]`。
    2.  **训练:** 随机掩码 (Mask) `Ẏ` 中的一部分标签（如设为 0），得到 `Ỹ`。使用 GNN 以 `[X, Ỹ]` 作为输入，预测被掩码的标签。这是一个自监督任务。
    3.  **推理:** 使用所有可见标签 `Ẏ`，即以 `[X, Ẏ]` 作为输入，用训练好的 GNN 预测无标签节点的标签。
*   **特点:** 将标签信息融入 GNN 的消息传递过程。也是一种自监督任务。




### 16 深度图生成模型 (Deep Generative Models for Graphs)

*   **目标:** 使用深度学习模型生成与真实图相似的“现实”图，或生成满足特定目标的图。
*   **动机:**
    *   **洞察 (Insights):** 理解图的形成机制。
    *   **预测 (Predictions):** 预测图的演化。
    *   **模拟 (Simulations):** 生成新的图实例（如新药物分子）。
    *   **异常检测 (Anomaly detection):** 检测图是否正常。
*   **历史:** 传统图生成模型（ER, 配置模型等）-> 深度图生成模型。
*   **核心:** 学习图的分布 `p_data(G)`，并从中采样生成新图 `G ~ p_model(G)`。通常涉及深度解码器：从潜在表示生成图结构。

#### 16.1 图生成的机器学习任务 (Machine Learning for Graph Generation)

*   **类型:**
    *   **现实图生成 (Realistic Graph Generation):** 生成与给定图集相似的图。
    *   **目标导向图生成 (Goal-directed graph generation):** 生成优化某个目标函数（如药物活性）或满足约束（如化学有效性）的图。

#### Excursion: 生成模型基础 (Basics of Generative Models)

*   **目标:** 从数据样本 `{xi}` 学习数据分布 `p_data(x)`。
*   **模型:** 参数化模型 `p_model(x|θ)`，可以从中采样。
*   **最大似然估计 (Maximum Likelihood):** 找到参数 `θ` 使得观测到样本 `{xi}` 的概率最大化。`θ* = argmax_θ Σ_i log p_model(xi|θ)`。
*   **采样方法:**
    *   **基于潜变量:** 从简单噪声分布（如高斯 `z ~ N(0, I)`）采样，通过函数 `f`（如神经网络）变换得到 `x = f(z|θ)`。`x` 的分布是噪声分布通过 `f` 的 pushforward。
    *   **自回归模型 (Auto-regressive models):** 序列化生成过程，利用链式法则 `p(x) = Π p(xt | x1..t-1)`。

#### 16.2 生成现实图 (Generating Realistic Graphs) - GraphRNN

*   **核心思想:** 将图生成过程分解为一系列节点和边的添加决策，使用递归神经网络 (RNN) 对这个序列决策过程建模。
*   **序列化:**
    1.  **节点排序:** 首先确定节点的添加顺序（如随机排序或 BFS 排序）。
    2.  **序列表示:** 一个图对应一个节点和边的添加序列 `(S^π_1, ..., S^π_n)`。
    3.  **边序列:** 每个 `S^π_i` 表示节点 `i` 与之前已存在节点 `{1, ..., i-1}` 之间的连接情况（邻接向量/矩阵的列）。`S^π_i = [S^π_{i,1}, ..., S^π_{i, i-1}]`。
*   **GraphRNN 架构:**
    *   **节点级 RNN (Node-level RNN):** 在宏观层面运行，每一步决定是否添加新节点，并为其对应的边级 RNN 生成初始状态。
    *   **边级 RNN (Edge-level RNN):** 在微观层面运行，对于每个新添加的节点 `i`，顺序地决定它是否与之前的节点 `j` (`j < i`) 相连。
*   **生成过程:**
    1.  节点级 RNN 生成状态，传递给边级 RNN。
    2.  边级 RNN 预测节点 `i` 与节点 `1, 2, ..., i-1` 的连接（输出 0/1 概率）。
    3.  边级 RNN 的最终状态反馈给节点级 RNN，用于下一步决策。
*   **训练:** 使用**教师强制 (Teacher Forcing)**。在训练时，RNN 的输入是真实图序列中的下一个元素，而不是模型自己上一步的输出。优化目标是预测下一个连接的交叉熵损失。使用**时间反向传播 (Backpropagation Through Time - BPTT)** 训练 RNN。
*   **局限性:**
    *   节点顺序依赖性：不同节点顺序对应不同序列，模型需要处理所有可能的排列，或者固定一种顺序（如 BFS）。
    *   长程依赖：为新节点预测与很久以前添加的节点的连接，需要 RNN 维持很长的记忆。

#### Excursion: 递归神经网络 (Recurrent Neural Networks - RNN)

*   **用途:** 处理序列数据。
*   **结构:** 具有隐藏状态 `s_t`，该状态在时间步之间传递。`s_t = σ(W x_t + U s_{t-1})`, `y_t = V s_t`。
*   **变体:** GRU, LSTM 用于缓解梯度消失问题。
*   **生成模式:** 将上一步输出 `y_{t-1}` 作为下一步输入 `x_t`。
*   **训练:** BPTT。

#### 16.3 扩展和评估图生成 (Scaling Up and Evaluating Graph Generation)

*   **扩展 GraphRNN:**
    *   **挑战:** 边级 RNN 需要 `O(n^2)` 步，对于大图不可行。长程依赖也困难。
    *   **BFS 节点排序:** 使用 BFS 序可以限制边级 RNN 需要回溯的步数。对于节点 `j`，只需考虑其 BFS 父节点和同一层的祖先节点，不需要考虑已经完成子树扩展的节点。回溯步数从 `n-1` 减少到 `max_v deg(v)` 或更小（取决于 BFS 实现）。
*   **评估图生成:**
    *   比较生成图与真实图的各种图统计量（如度分布、聚类系数分布、模体计数）。
    *   使用 MMD (Maximum Mean Discrepancy) 等度量比较图特征的分布。

#### 16.4 图卷积策略网络 (Graph Convolutional Policy Network - GCPN)

*   **目标:** 生成满足特定目标的图（目标导向）。
*   **框架:** 结合 GNN 和强化学习 (Reinforcement Learning - RL)。
*   **与 GraphRNN 的区别:**
    *   GCPN 使用 GNN 来预测生成动作（更具表达力，但计算更慢）。
    *   GCPN 使用 RL 来直接优化目标函数（可以是黑盒）。
*   **生成步骤 (类似 GraphRNN，但动作由 GNN+RL 决定):**
    (a) 添加节点。
    (b, c) 使用 GNN 预测连接到哪些现有节点。
    (d) 根据预测（策略）采取连接动作。
    (e, f) 计算奖励 (Reward)。
*   **奖励:**
    *   **中间奖励 (Step Reward):** 鼓励生成有效/合法的结构（如化学有效性）。
    *   **最终奖励 (Final Reward):** 基于最终生成图的目标属性（如药物活性预测得分）。
*   **训练:**
    1.  **监督预训练:** 使用真实图数据模仿专家行为（模仿学习）。
    2.  **RL 训练:** 使用策略梯度 (Policy Gradient) 算法，根据奖励信号优化 GNN 策略网络。

#### Excursion: 强化学习 (Reinforcement Learning - RL)

*   **框架:** Agent 在 Environment 中观察状态，采取 Action，获得 Reward，进入新状态。
*   **目标:** Agent 学习一个策略 (Policy) 来最大化累积奖励。
*   **环境:** 可以是黑盒。




### 17 图神经网络高级主题 (Advanced Topics in Graph Neural Networks)

#### 17.1 鲁棒性 (Robustness)

*   **背景:** DNN 容易受到对抗性攻击 (Adversarial Attacks)。微小的、人难以察觉的输入扰动可能导致模型预测错误。
*   **GNN 的鲁棒性:** GNN 是否也容易受到攻击？
*   **攻击设置 (节点分类):**
    *   **目标:** 改变目标节点 `t` 的预测标签。
    *   **攻击者能力:** 可以修改图结构（增删边）、节点特征。修改应尽可能小（不易察觉）。攻击者知道模型结构、参数、原始数据。
    *   **攻击类型:**
        *   **直接攻击 (Direct Attack):** 攻击者可以修改目标节点 `t` 或其连接/特征 (`t ∈ S`, S 是可修改节点集)。
        *   **间接攻击 (Indirect Attack):** 攻击者只能修改非目标节点 (`t ∉ S`)。
    *   **优化目标:** 最大化预测改变的程度（如目标类别与原始预测类别的 logit 差异 `Δ(v; A', X')`），同时满足扰动约束 `(A', X') ≈ (A, X)`。
*   **挑战:** 图结构是离散的，优化困难；每次修改都需要重新训练 GNN（如果攻击者不知道参数）。
*   **攻击策略 (以 Nettack 为例):**
    *   迭代地、贪心地修改图结构/特征。
    *   每次选择能最大化 `Δ` 梯度（或近似梯度）的单次修改（增/删边、改特征）。
    *   不需要重新训练模型（通过 GCN 的线性化近似梯度）。
*   **结果:** GCN 对直接攻击不鲁棒，但对间接攻击和随机攻击有一定鲁棒性。
*   **防御:** 对抗训练、认证防御等。

#### Excursion: 对抗性攻击 (Adversarial Attacks)

*   **解释:** 对输入数据添加精心设计的、通常难以察觉的扰动，使得机器学习模型产生错误的输出。
*   **例子:** 在熊猫图片上添加微小噪声，导致模型将其分类为长臂猿。




### 18 扩展 GNN (Scaling Up GNNs)

*   **动机:** 真实世界的图（社交网络、推荐系统、知识图谱）规模巨大，节点/边数量可达数十亿甚至万亿。全图 GNN 训练需要巨大内存（存储节点嵌入和计算图），标准 GPU 无法容纳。
*   **挑战:** 如何在有限的硬件资源下训练 GNN？
*   **扩展方法:**
    1.  **邻居采样 (Neighbor Sampling):**
        *   **思想:** 计算节点 `n` 的嵌入时，不需要其完整的 K-hop 邻域，只需采样一部分邻居。
        *   **方法:** 在 GNN 的每一层聚合时，不是聚合所有邻居，而是随机采样固定数量 (`H_k`) 的邻居。
        *   **计算图:** 构建一个剪枝后的计算图进行训练。一个 K 层 GNN 的计算图最多涉及 `Π H_k` 个叶节点。
        *   **优点:** 显著减少计算量和内存需求。
        *   **缺点:** 增加了估计方差（导致训练不稳定），性能可能受影响。`H` 是效率和方差的权衡。计算图大小仍可能随层数 K 指数增长。
        *   **采样策略:** 均匀随机采样；基于随机游走（如 PinSAGE 使用 random walk with restart）采样重要邻居（效果更好）。
        *   **时间复杂度:** `O(M * Π H^K)` (M 是 batch size)。
    2.  **Cluster-GCN:**
        *   **观察:** 全批量 GNN 计算中存在冗余，许多节点共享邻居，它们的嵌入更新可以复用。
        *   **思想:** 将图划分成多个子图（簇, Cluster），每次加载一个或多个子图到 GPU 进行 GNN 计算。利用社区结构使子图内部连接密集，跨子图连接稀疏。
        *   **步骤:**
            1.  **预处理 (图划分):** 使用图聚类算法（如 Louvain, METIS）将节点 V 划分为 C 个簇 `V1, ..., VC`。簇的大小应能放入 GPU。
            2.  **Mini-batch 训练:** 每次采样一个簇 `Vi`，在其诱导子图 `Gi` 上执行标准的 GNN 前向和反向传播。
        *   **问题 (基础版):**
            *   **丢弃簇间边:** 导致梯度估计有偏。
            *   **聚类质量:** 采样单个簇可能无法代表全局信息，导致梯度方差大，收敛慢。
        *   **高级 Cluster-GCN:**
            *   **Mini-batch:** 每次采样 **多个** 簇 `{Vi1, ..., Viq}`，合并成一个子图 `G_batch` (包含这些簇内的节点和边，以及**它们之间的边**)。
            *   在 `G_batch` 上执行 GNN 训练。
        *   **优点:** 计算更集中，利用了数据局部性；通过合并簇缓解了边界截断问题；梯度更稳定。
        *   **时间复杂度:** `O(K * M * D_avg)` (M 是 batch size 内节点数, `D_avg` 是平均度)，比邻居采样更高效。
    3.  **简化 GCN (Simplified GCN - SGC):**
        *   **思想:** 移除 GCN 中的非线性激活函数，并将多个权重矩阵合并为一个。
        *   **模型:** `Ẽ = Ã^K E W`。其中 `Ã = D̃^(-1/2) Ã D̃^(-1/2)` (`Ã = A+I`) 是归一化邻接矩阵，`E` 是初始节点特征，`W` 是一个线性分类器权重。
        *   **计算:** `X' = Ã^K E` 可以**预计算**一次，得到平滑后的节点特征 `X'`。
        *   **训练:** 只需在预计算得到的 `X'` 上训练一个简单的线性分类器 `Y = softmax(X' W)`。
        *   **优点:** 极其快速和可扩展，因为 GNN 部分变成了预处理步骤。
        *   **缺点:** 表达能力大大降低（模型是线性的）。
        *   **适用场景:** 当图中存在强烈的**同质性**（连接的节点倾向于有相同标签）时，SGC 效果较好。因为 `Ã^K E` 本质上是在做特征平滑/传播。




### 19 几何图学习 (Geometric Graph Learning)

*   **背景:** 很多图数据（如分子、蛋白质）具有内在的几何结构，节点在 2D 或 3D 空间中具有坐标。
*   **几何图 (Geometric Graph):** `G = (A, S, R)`。`A` 邻接矩阵，`S` 节点标量特征，`R ∈ R^(|V|×d)` 节点坐标。
*   **挑战:** 如何设计 GNN 来处理和利用这些几何信息？坐标信息会随着坐标系的刚性变换（旋转、平移）而改变，但图的内在属性（如能量）或某些预测任务（如力）应该具有相应的**不变性 (Invariance)** 或 **等变性 (Equivariance)**。
*   **对称性:**
    *   **刚性变换 (Rigid Transformation) ρ:** 保持距离和手性的变换（旋转、平移）。在 3D 中对应于 SE(3) 群。
    *   **不变性 (Invariant):** 函数 `F` 满足 `F(ρX(G)) = F(G)`。例如，分子的能量是 SE(3) 不变的。
    *   **等变性 (Equivariant):** 函数 `F` 满足 `F(ρX(G)) = ρY(F(G))`。例如，作用在原子上的力是 SE(3) 等变的（坐标系旋转，力的矢量也相应旋转）。
*   **数据增强 vs. 内建对称性:**
    *   **数据增强:** 通过随机刚性变换扩充数据。计算成本高，不能完全保证对称性。
    *   **内建对称性:** 设计 GNN 架构本身就尊重 SE(3) 对称性。可以显著缩小学习的搜索空间。
*   **两类几何 GNN:**
    *   **不变 GNN (Invariant GNNs):** 用于学习不变的标量特征。
    *   **等变 GNN (Equivariant GNNs):** 用于学习等变的张量（如矢量）特征。

#### 19.1 不变 GNN (Invariant GNNs)

*   **思想:** GNN 的消息传递和聚合应该只依赖于节点间的相对距离和角度等 SE(3) 不变量。
*   **连续滤波器卷积 (Continuous-Filter Convolutions):**
    *   **消息:** 节点 `j` 的消息 `h_j^(l)`。
    *   **滤波器 (Filter) W:** 卷积核 `W^(l)` 是一个函数，其值依赖于节点间的相对位置 `ri - rj`。`W^(l): R^d -> R^f`。
    *   **卷积:** `h_i^(l+1) = Σ_j h_j^(l) ⊙ W^(l)(ri - rj)` (⊙ 是逐元素乘法或其他组合)。
*   **SchNet:**
    *   **滤波器:** 将 `W^(l)` 设计为**仅依赖于距离 `d_ij = ||ri - rj||`** 的函数（通过 MLP 作用在距离的径向基函数 `exp(-γ(d_ij - μ_k)^2)` 上）。这使得卷积操作对旋转不变。
    *   **局限性:** 只使用距离信息，丢失了角度信息，无法区分某些几何异构体（如顺反异构体）。
*   **DimeNet:**
    *   改进 SchNet，同时使用**距离和角度**信息进行消息传递，提高了表达能力。

#### 19.2 等变 GNN (Equivariant GNNs)

*   **目标:** 处理需要预测矢量或其他张量属性的任务（如力预测、分子构象生成）。网络的中间表示和最终输出都需要具备 SE(3) 等变性。
*   **PaiNN (Polarizable Atomic Interaction Neural Network):**
    *   **节点表示:** 每个节点 `i` 维护两种特征：
        *   标量特征 `s_i^(l)` (SE(3) 不变)。
        *   矢量特征 `v_i^(l)` (SE(3) 等变)。
    *   **更新规则:** 设计消息传递和更新规则，使得 `s_i^(l+1)` 保持不变性，`v_i^(l+1)` 保持等变性。
        *   `Δs_i^(l)`: 由节点间相对距离 `||r_ij||` 和标量/矢量特征计算得到（不变）。
        *   `Δv_i^(l)`: 由节点间相对距离 `||r_ij||`、相对方向向量 `r_ij / ||r_ij||` 和标量/矢量特征计算得到（等变）。
        *   `s_i^(l+1) = s_i^(l) + Δs_i^(l)`
        *   `v_i^(l+1) = v_i^(l) + Δv_i^(l)`
*   **关键:** 通过精心设计的函数 (`ϕ`, `W`) 和利用相对距离/方向，确保更新过程尊重 SE(3) 对称性。

#### 19.3 几何生成模型 (Geometric Generative Models)

*   **应用:** 分子/蛋白质设计、生物分子结构预测、药物对接模拟。
*   **分子构象生成 (Molecular Conformation Generation):**
    *   **目标:** 给定分子图 `G`（原子和键），生成其稳定的 3D 构象 `C`（原子坐标）。
    *   **物理背景:** 构象 `C` 服从玻尔兹曼分布 `P(C) ∝ exp(-E(C)/T)`，低能量构象概率高。
*   **几何扩散模型 (Geometric Diffusion - GeoDiff):**
    *   **思想:** 使用扩散模型生成分子构象。
    *   **前向过程 (破坏):** 从真实构象 `C^(0)` 开始，逐步添加高斯噪声，得到一系列加噪构象 `C^(t)`。`q(C^(t)|C^(t-1))` 是加噪过程。
    *   **反向过程 (生成/去噪):** 学习一个去噪网络 `p_θ(C^(t-1)|G, C^(t))`，从噪声构象 `C^(T)` (纯噪声) 和分子图 `G` 出发，逐步去噪，生成目标构象 `C^(0)`。
    *   **关键:** 去噪网络 `p_θ` 需要是 **SE(3) 等变的 GNN**，因为它处理的是坐标信息 `C^(t)`。

#### Excursion: 扩散模型 (Diffusion Models)

*   **解释:** 一类生成模型。通过模拟一个逐渐破坏数据（加噪）的正向过程和一个学习逆转该过程（去噪）的反向过程来生成数据。




### 20 可信赖图 AI (Trustworthy Graph AI)

*   **范围:** 包括可解释性 (Explainability)、公平性 (Fairness)、鲁棒性 (Robustness)、隐私性 (Privacy) 等。
*   **本讲重点:** 鲁棒性 (已在 [[高级主题]] 讨论) 和可解释性。

#### 20.1 可解释性 (Explainability)

*   **动机:** 深度学习模型通常是“黑箱”，难以理解其内部决策逻辑。可解释人工智能 (Explainable Artificial Intelligence - XAI) 旨在解决此问题。
*   **重要性:**
    *   **信任 (Trust):** 人类需要理解模型才能信任和接受其预测。
    *   **因果性 (Causality):** 解释有助于理解输入属性与预测结果之间的因果关系。
    *   **可迁移性 (Transferability):** 理解决策机制有助于安全地将模型部署到新环境。
    *   **公平与伦理 (Fair and Ethical Decision Making):** 了解决策原因有助于判断是否符合伦理标准。
*   **可解释模型 vs. 事后解释:**
    *   **可解释模型 (Explainable Models):** 模型本身结构简单或具有内在的可解释性。
        *   **例子:** 线性回归（权重即重要性），决策树（路径即规则），降维（可视化）。
    *   **事后解释 (Post-hoc Explanation):** 对已训练好的（可能是黑箱）模型进行解释。
        *   **代理模型 (Proxy Model):** 用一个简单的可解释模型局部近似原模型。
        *   **显著图 (Saliency Map):** 计算输出对输入的梯度，表示输入特征的重要性。
        *   **注意力权重:** 可视化 Transformer 或 GAT 中的注意力权重。
*   **解释的层面:**
    *   **实例级 (Instance-level):** 解释单个输入 `x` 的预测 `ŷ`。
    *   **模型级 (Model-level):** 解释模型在整个数据集 `D` 或某个类别上的全局行为。
*   **解释的阶段:**
    *   **事前 (Ante-hoc):** 模型本身内在可解释。
    *   **事后 (Post-hoc):** 解释已训练好的模型。
*   **解释的方法:**
    *   **模型特定 (Model-specific):** 解释方法仅适用于特定类型的模型。
    *   **模型无关 (Model-agnostic):** 解释方法适用于任何模型。

#### 图学习中的解释

*   **目标:** 识别对 GNN 预测起关键作用的**重要子图结构**和**重要节点特征子集**。
*   **例子:**
    *   解释现象：哪些分子结构特征导致毒性？
    *   解释预测：为什么模型拒绝了用户 X 的贷款申请？

#### 20.2 GNNExplainer

*   **类型:** 事后 (Post-hoc)、模型无关 (Model-agnostic) 的 GNN 解释方法。
*   **目标:** 为 GNN 的预测（节点分类、链接预测、图分类）找到一个重要的子图结构和相关的节点特征。
*   **输入:** 预训练好的 GNN `ϕ`，待解释实例（如节点 `v` 及其计算图 `Gc(v)`）。
*   **输出:**
    *   邻接矩阵掩码 `M`：指示哪些边重要。解释子图 `Â = Ac ⊙ σ(M)`。
    *   特征掩码 `F`：指示哪些特征维度重要。解释特征 `X̂^F`。
*   **核心思想:** 最大化解释 `(Â, X̂^F)` 与模型预测 `Y` 之间的**互信息 (Mutual Information)**。
    `max I(Y, (Â, X̂^F)) = H(Y) - H(Y | A = Â, X = X̂^F)`
    等价于最小化条件熵 `H(Y | A = Â, X = X̂^F)`。
*   **优化:**
    *   直接优化离散的子图 `Â` 是 NP 难的。
    *   **连续松弛:** 优化连续的掩码 `M` 和 `F`。`Â` 近似为 `Ac ⊙ σ(M)`，`X̂^F` 通过 `σ(F)` 选择特征。
    *   **目标函数:** `min_{M,F} - H(Pϕ(Y | A=Ac⊙σ(M), X=X̂^F))` (近似互信息)。
    *   **正则化:** 添加掩码稀疏性惩罚项（如 L1 或熵），鼓励解释简洁。`+ λ1 ||σ(M)|| + λ2 ||σ(F)||`。
*   **优化过程:** 使用梯度下降优化掩码 `M` 和 `F`。
*   **最终解释:** 对优化得到的 `σ(M)` 和 `σ(F)` 进行阈值处理，得到离散的子图和特征子集。
*   **适用任务:**
    *   节点分类: 优化 `(M, F)` 在节点 `v` 的计算图（邻域）上。
    *   图分类: 优化 `(M, F)` 在整个图上。

#### 20.3 可解释性评估 (Explainability Evaluation)

*   **挑战:** 通常没有 ground truth 解释。评估是多维度的。
*   **评估指标:**
    *   **保真度 (Fidelity):** 解释与模型预测的一致性。
        *   **Fidelity+ (充分性 Sufficiency):** 只使用解释（重要子图/特征）时，模型预测与原预测的相似度。越高越好 (`fid- → 0`)。
        *   **Fidelity- (必要性 Necessity):** 移除解释（重要子图/特征）后，模型预测发生改变的程度。越高越好 (`fid+ → 1`)。
        *   **特性得分 (Characterization Score):** 结合 `fid+` 和 `fid-` 的综合指标 `c = (w+ + w-) * fid+ * (1 - fid-) / (w+(1 - fid-) + w- * fid+)`。
    *   **稳定性 (Stability):** 多次运行解释方法，结果的一致性。
    *   **复杂度/简洁性 (Complexity):** 解释的大小（边数、特征数）。越简洁越好。
*   **其他解释类型:**
    *   **反事实解释 (Counterfactual Explanations):** 需要对输入做何种最小改动才能改变预测结果？
    *   **模型级别解释 (Model-Level Explanations):** 属于某个特定类别的所有实例具有哪些共同特征？




### 21 结论 (Conclusion)

#### 21.1 GNN 设计空间和任务空间 (GNN Design Space and Task Space)

*   **挑战:** 如何为特定 GNN 任务找到好的 GNN 设计？手动进行超参数搜索成本高。
*   **概念:**
    *   **设计 (Design):** 具体的 GNN 模型实例 (如 4 层 GraphSAGE)。
    *   **设计维度 (Design Dimension):** 表征设计的参数 (如层数 `l ∈ {2, 4, 6, 8}`)。
    *   **设计选择 (Design choice):** 设计维度的具体取值 (如 `l=2`)。
    *   **设计空间 (Design space):** 所有设计维度选择的笛卡尔积。极其巨大。
    *   **任务 (Task):** 具体目标 (如 Cora 节点分类)。
    *   **任务空间 (Task Space):** 所有关心的任务集合。
*   **GNN 设计空间维度示例:**
    *   **层内设计 (Intra-Layer):** 聚合函数，激活函数，Dropout 率，是否使用 Batch Normalization。
    *   **层间设计 (Inter-Layer):** 层连接方式（跳跃连接），预处理/后处理层层数。
    *   **学习配置 (Learning-Configuration):** Batch size, 学习率，优化器，训练轮数。
*   **GraphGym:** 一个用于探索 GNN 设计空间和任务空间的平台。

#### 21.2 GraphGym

*   **目标:** 系统性地研究 GNN 设计，理解不同设计选择的影响，发现跨任务的通用设计原则。
*   **任务相似性度量:**
    1.  选择一组**锚点模型 (Anchor Models)** (如 5 个)。
    2.  对于每个任务，评估锚点模型的性能并排序。
    3.  任务的特征由锚点模型的性能排名向量表示。
    4.  两个任务的相似性通过它们排名向量的相似性（如斯皮尔曼等级相关系数）来衡量。
*   **应用:**
    *   **评估设计维度:** 通过在大量任务上采样大量模型，评估某个设计选择（如使用 Batch Normalization）的平均影响。
    *   **任务迁移:** 如果新任务与某个已知任务相似，可以将已知任务上的最佳设计推荐给新任务。
*   **发现:** 不同的任务类型（如节点 vs. 图分类，结构依赖 vs. 特征依赖）确实偏好不同的 GNN 设计。

#### 21.3 预训练图神经网络 (Pre-Training Graph Neural Networks)

*   **动机:** 在许多（特别是科学）领域，有标签的图数据稀缺，但无标签数据丰富。直接在小标签数据集上训练深度 GNN 容易过拟合，且泛化能力差（特别是分布外 OOD 泛化）。
*   **策略:**
    *   **预训练 (Pre-training):** 在相关的、大规模的数据集（可以无标签或有其他类型标签）上训练 GNN 模型，学习通用的图表示能力。
    *   **微调 (Fine-tuning):** 将预训练好的模型在目标下游任务的小标签数据集上进行少量训练调整。
*   **图预训练的挑战:** 如何设计有效的预训练任务？
*   **预训练方法 (分子图为例):**
    *   **朴素策略 (监督多任务):** 在大量（可能上千个）相关的二元分类任务上进行多任务学习预训练。
        *   **局限性:** 性能提升有限，易发生负迁移（不同任务冲突）。
    *   **结合节点和图级别预训练 (关键思想):** 同时进行节点级别和图级别的自监督预训练。
        *   **属性掩码 (Attribute Masking - 节点级, 自监督):**
            1.  随机掩码一部分节点的原子类型/属性。
            2.  使用 GNN 基于周围结构预测被掩码的属性。迫使 GNN 理解原子层面的化学知识。
        *   **上下文预测 (Context Prediction - 节点级, 自监督):**
            1.  采样中心节点，提取其 K-hop 邻域（Neighborhood）和周围环境（Context，不含中心节点的更大范围子图）。
            2.  使用 GNN 分别编码邻域和上下文图。
            3.  优化目标：最大化来自同一原始图的 (邻域, 上下文) 对的嵌入内积，最小化来自不同图的内积。迫使 GNN 理解子图与其环境的关系，捕捉语义相似性。
        *   **监督属性预测 (Supervised Attribute Prediction - 图级):** 在大量已知的分子属性标签上进行多任务监督预训练。
        *   **结构相似性预测 (Structural Similarity Prediction - 图级):** 预测两个图之间的结构相似性（如图编辑距离等）。
*   **结果:** 结合多种自监督和监督预训练任务，尤其是在节点和图层面同时进行预训练，可以显著提升模型在下游任务（特别是 OOD 任务）上的性能，避免负迁移。最强大的基础 GNN 模型（如 GIN）从预训练中获益最多。




